{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 256\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Constants\n",
    "GAMMA = 0.99\n",
    "num_steps = 300\n",
    "max_episodes = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.critic_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.critic_linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.actor_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.actor_linear2 = nn.Linear(hidden_size, num_actions)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        value = F.relu(self.critic_linear1(state))\n",
    "        value = self.critic_linear2(value)\n",
    "        \n",
    "        policy_dist = F.relu(self.actor_linear1(state))\n",
    "        policy_dist = F.softmax(self.actor_linear2(policy_dist), dim=1)\n",
    "\n",
    "        return value, policy_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2c(env):\n",
    "    num_inputs = env.observation_space.shape[0]\n",
    "    num_outputs = env.action_space.n\n",
    "    \n",
    "    actor_critic = ActorCritic(num_inputs, num_outputs, hidden_size)\n",
    "    ac_optimizer = optim.Adam(actor_critic.parameters(), lr=learning_rate)\n",
    "\n",
    "    all_lengths = []\n",
    "    average_lengths = []\n",
    "    all_rewards = []\n",
    "    entropy_term = 0\n",
    "\n",
    "    for episode in range(max_episodes):\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "\n",
    "        state = env.reset()\n",
    "        for steps in range(num_steps):\n",
    "            value, policy_dist = actor_critic.forward(state)\n",
    "            value = value.detach().numpy()[0,0]\n",
    "            dist = policy_dist.detach().numpy() \n",
    "\n",
    "            action = np.random.choice(num_outputs, p=np.squeeze(dist))\n",
    "            log_prob = torch.log(policy_dist.squeeze(0)[action])\n",
    "            entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            log_probs.append(log_prob)\n",
    "            entropy_term += entropy\n",
    "            state = new_state\n",
    "            \n",
    "            if done or steps == num_steps-1:\n",
    "                Qval, _ = actor_critic.forward(new_state)\n",
    "                Qval = Qval.detach().numpy()[0,0]\n",
    "                all_rewards.append(np.sum(rewards))\n",
    "                all_lengths.append(steps)\n",
    "                average_lengths.append(np.mean(all_lengths[-10:]))\n",
    "                if episode % 10 == 0:                    \n",
    "                    sys.stdout.write(\"episode: {}, reward: {}, total length: {}, average length: {} \\n\".format(episode, np.sum(rewards), steps, average_lengths[-1]))\n",
    "                break\n",
    "        \n",
    "        # compute Q values\n",
    "        Qvals = np.zeros_like(values)\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            Qval = rewards[t] + GAMMA * Qval\n",
    "            Qvals[t] = Qval\n",
    "  \n",
    "        #update actor critic\n",
    "        values = torch.FloatTensor(values)\n",
    "        Qvals = torch.FloatTensor(Qvals)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        \n",
    "        advantage = Qvals - values\n",
    "        actor_loss = (-log_probs * advantage).mean()\n",
    "        critic_loss = 0.5 * advantage.pow(2).mean()\n",
    "        ac_loss = actor_loss + critic_loss + 0.001 * entropy_term\n",
    "\n",
    "        ac_optimizer.zero_grad()\n",
    "        ac_loss.backward()\n",
    "        ac_optimizer.step()\n",
    "\n",
    "        \n",
    "    \n",
    "    # Plot results\n",
    "    smoothed_rewards = pd.Series.rolling(pd.Series(all_rewards), 10).mean()\n",
    "    smoothed_rewards = [elem for elem in smoothed_rewards]\n",
    "    plt.plot(all_rewards)\n",
    "    plt.plot(smoothed_rewards)\n",
    "    plt.plot()\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(all_lengths)\n",
    "    plt.plot(average_lengths)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Episode length')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make(\"Pendulum-v0\").env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, reward: 24.0, total length: 23, average length: 23.0 \n",
      "episode: 10, reward: 17.0, total length: 16, average length: 22.8 \n",
      "episode: 20, reward: 25.0, total length: 24, average length: 25.4 \n",
      "episode: 30, reward: 27.0, total length: 26, average length: 20.7 \n",
      "episode: 40, reward: 11.0, total length: 10, average length: 22.6 \n",
      "episode: 50, reward: 13.0, total length: 12, average length: 20.5 \n",
      "episode: 60, reward: 15.0, total length: 14, average length: 25.9 \n",
      "episode: 70, reward: 29.0, total length: 28, average length: 24.7 \n",
      "episode: 80, reward: 15.0, total length: 14, average length: 20.3 \n",
      "episode: 90, reward: 20.0, total length: 19, average length: 29.2 \n",
      "episode: 100, reward: 26.0, total length: 25, average length: 18.8 \n",
      "episode: 110, reward: 13.0, total length: 12, average length: 41.7 \n",
      "episode: 120, reward: 40.0, total length: 39, average length: 27.7 \n",
      "episode: 130, reward: 20.0, total length: 19, average length: 34.7 \n",
      "episode: 140, reward: 17.0, total length: 16, average length: 19.9 \n",
      "episode: 150, reward: 23.0, total length: 22, average length: 25.3 \n",
      "episode: 160, reward: 13.0, total length: 12, average length: 19.6 \n",
      "episode: 170, reward: 77.0, total length: 76, average length: 36.2 \n",
      "episode: 180, reward: 29.0, total length: 28, average length: 25.6 \n",
      "episode: 190, reward: 31.0, total length: 30, average length: 44.7 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-81830b528250>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CartPole-v0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma2c\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-9f1d52a3a582>\u001b[0m in \u001b[0;36ma2c\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mactor_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlog_probs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0madvantage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mcritic_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0madvantage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mac_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcritic_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.001\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mentropy_term\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mac_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "a2c(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
