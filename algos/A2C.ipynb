{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/abhisheksuran/Atari_DQN/blob/master/A2C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "udtLxTQyiNUz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CloudPickler' from 'cloudpickle.cloudpickle' (C:\\Users\\User\\.conda\\envs\\rltrading\\lib\\site-packages\\cloudpickle\\cloudpickle.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-705e502d67ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mkls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_probability\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;31m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;31m# pylint: enable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_probability\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbijectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_probability\\python\\distributions\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpareto\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPareto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpert\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPERT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpixel_cnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPixelCNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplackett_luce\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPlackettLuce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoisson\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPoisson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_probability\\python\\distributions\\pixel_cnn.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreparameterization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorshape_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mweight_norm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_probability\\python\\layers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_variational\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDenseReparameterization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_variational_v2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDenseVariational\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCategoricalMixtureOfOneHotCategorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDistributionLambda\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIndependentBernoulli\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_probability\\python\\layers\\distribution_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Dependency imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcloudpickle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'CloudPickler' from 'cloudpickle.cloudpickle' (C:\\Users\\User\\.conda\\envs\\rltrading\\lib\\site-packages\\cloudpickle\\cloudpickle.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import gym\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.losses as kls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "nfKWiIG6m5Ed",
    "outputId": "e6b2bcea-5ddd-49b1-e8f2-306cb41e42fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting box2d-py\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
      "\r",
      "\u001b[K     |▊                               | 10kB 25.3MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 20kB 3.1MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 30kB 4.1MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 40kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 51kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 61kB 4.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 71kB 4.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 81kB 4.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 92kB 4.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 102kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 112kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 122kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 133kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 143kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 153kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 163kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 174kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 184kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 194kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 204kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 215kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 225kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 235kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 245kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 256kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 266kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 276kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 286kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 296kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 307kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 317kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 327kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 337kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 348kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 358kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 368kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 378kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 389kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 399kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 409kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 419kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 430kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 440kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 450kB 4.8MB/s \n",
      "\u001b[?25hInstalling collected packages: box2d-py\n",
      "Successfully installed box2d-py-2.3.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip3 install box2d-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4X3S4uD_m-e_"
   },
   "outputs": [],
   "source": [
    "env= gym.make(\"CartPole-v0\")\n",
    "low = env.observation_space.low\n",
    "high = env.observation_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pAZ_YRduimE0"
   },
   "outputs": [],
   "source": [
    "class critic(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.d1 = tf.keras.layers.Dense(128,activation='relu')\n",
    "    #self.d2 = tf.keras.layers.Dense(32,activation='relu')\n",
    "    self.v = tf.keras.layers.Dense(1, activation = None)\n",
    "\n",
    "  def call(self, input_data):\n",
    "    x = self.d1(input_data)\n",
    "    #x = self.d2(x)\n",
    "    v = self.v(x)\n",
    "    return v\n",
    "    \n",
    "\n",
    "class actor(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.d1 = tf.keras.layers.Dense(128,activation='relu')\n",
    "    #self.d2 = tf.keras.layers.Dense(32,activation='relu')\n",
    "    self.a = tf.keras.layers.Dense(2,activation='softmax')\n",
    "\n",
    "  def call(self, input_data):\n",
    "    x = self.d1(input_data)\n",
    "    #x = self.d2(x)\n",
    "    a = self.a(x)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0-5_oxXmwT9"
   },
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, gamma = 0.99):\n",
    "        self.gamma = gamma\n",
    "        self.a_opt = tf.keras.optimizers.RMSprop(learning_rate=7e-3)\n",
    "        self.c_opt = tf.keras.optimizers.RMSprop(learning_rate=7e-3)\n",
    "        self.actor = actor()\n",
    "        self.critic = critic()\n",
    "\n",
    "          \n",
    "    def act(self,state):\n",
    "        prob = self.actor(np.array([state]))\n",
    "        prob = prob.numpy()\n",
    "        dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
    "        action = dist.sample()\n",
    "        return int(action.numpy()[0])\n",
    "  \n",
    "\n",
    "\n",
    "    def actor_loss(self, probs, actions, td):\n",
    "        \n",
    "        probability = []\n",
    "        log_probability= []\n",
    "        for pb,a in zip(probs,actions):\n",
    "          dist = tfp.distributions.Categorical(probs=pb, dtype=tf.float32)\n",
    "          log_prob = dist.log_prob(a)\n",
    "          prob = dist.prob(a)\n",
    "          probability.append(prob)\n",
    "          log_probability.append(log_prob)\n",
    "\n",
    "        # print(probability)\n",
    "        # print(log_probability)\n",
    "\n",
    "        p_loss= []\n",
    "        e_loss = []\n",
    "        td = td.numpy()\n",
    "        #print(td)\n",
    "        for pb, t, lpb in zip(probability, td, log_probability):\n",
    "                        t =  tf.constant(t)\n",
    "                        policy_loss = tf.math.multiply(lpb,t)\n",
    "                        entropy_loss = tf.math.negative(tf.math.multiply(pb,lpb))\n",
    "                        p_loss.append(policy_loss)\n",
    "                        e_loss.append(entropy_loss)\n",
    "        p_loss = tf.stack(p_loss)\n",
    "        e_loss = tf.stack(e_loss)\n",
    "        p_loss = tf.reduce_mean(p_loss)\n",
    "        e_loss = tf.reduce_mean(e_loss)\n",
    "        # print(p_loss)\n",
    "        # print(e_loss)\n",
    "        loss = -p_loss - 0.0001 * e_loss\n",
    "        #print(loss)\n",
    "        return loss\n",
    "\n",
    "    def learn(self, states, actions, discnt_rewards):\n",
    "        discnt_rewards = tf.reshape(discnt_rewards, (len(discnt_rewards),))\n",
    "        \n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            p = self.actor(states, training=True)\n",
    "            v =  self.critic(states,training=True)\n",
    "            v = tf.reshape(v, (len(v),))\n",
    "            td = tf.math.subtract(discnt_rewards, v)\n",
    "            # print(discnt_rewards)\n",
    "            # print(v)\n",
    "            #print(td.numpy())\n",
    "            a_loss = self.actor_loss(p, actions, td)\n",
    "            c_loss = 0.5*kls.mean_squared_error(discnt_rewards, v)\n",
    "        grads1 = tape1.gradient(a_loss, self.actor.trainable_variables)\n",
    "        grads2 = tape2.gradient(c_loss, self.critic.trainable_variables)\n",
    "        self.a_opt.apply_gradients(zip(grads1, self.actor.trainable_variables))\n",
    "        self.c_opt.apply_gradients(zip(grads2, self.critic.trainable_variables))\n",
    "        return a_loss, c_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vk4Phsi1tvS-",
    "outputId": "8849de20-a09f-44c1-ac77-550b47d3400b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer actor_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "total reward after 0 steps is 24.0 and avg reward is 24.0\n",
      "al8.580991744995117\n",
      "cl101.52865600585938\n",
      "total reward after 1 steps is 18.0 and avg reward is 21.0\n",
      "al6.569655418395996\n",
      "cl54.59226989746094\n",
      "total reward after 2 steps is 30.0 and avg reward is 24.0\n",
      "al10.033202171325684\n",
      "cl147.477294921875\n",
      "total reward after 3 steps is 11.0 and avg reward is 20.75\n",
      "al3.4086062908172607\n",
      "cl18.402528762817383\n",
      "total reward after 4 steps is 16.0 and avg reward is 19.8\n",
      "al5.239793300628662\n",
      "cl38.403438568115234\n",
      "total reward after 5 steps is 13.0 and avg reward is 18.666666666666668\n",
      "al3.9902877807617188\n",
      "cl23.384889602661133\n",
      "total reward after 6 steps is 28.0 and avg reward is 20.0\n",
      "al8.418906211853027\n",
      "cl117.70780181884766\n",
      "total reward after 7 steps is 18.0 and avg reward is 19.75\n",
      "al4.59422492980957\n",
      "cl43.99668502807617\n",
      "total reward after 8 steps is 29.0 and avg reward is 20.77777777777778\n",
      "al9.125882148742676\n",
      "cl120.15880584716797\n",
      "total reward after 9 steps is 21.0 and avg reward is 20.8\n",
      "al5.84741735458374\n",
      "cl56.198387145996094\n",
      "total reward after 10 steps is 55.0 and avg reward is 23.90909090909091\n",
      "al16.4592227935791\n",
      "cl443.32391357421875\n",
      "total reward after 11 steps is 11.0 and avg reward is 22.833333333333332\n",
      "al1.8425061702728271\n",
      "cl10.459774017333984\n",
      "total reward after 12 steps is 9.0 and avg reward is 21.76923076923077\n",
      "al0.8429644107818604\n",
      "cl6.616237163543701\n",
      "total reward after 13 steps is 21.0 and avg reward is 21.714285714285715\n",
      "al4.920080661773682\n",
      "cl50.29084014892578\n",
      "total reward after 14 steps is 16.0 and avg reward is 21.333333333333332\n",
      "al2.9993832111358643\n",
      "cl21.888696670532227\n",
      "total reward after 15 steps is 48.0 and avg reward is 23.0\n",
      "al14.206459999084473\n",
      "cl315.5088806152344\n",
      "total reward after 16 steps is 28.0 and avg reward is 23.294117647058822\n",
      "al6.250636577606201\n",
      "cl84.20174407958984\n",
      "total reward after 17 steps is 45.0 and avg reward is 24.5\n",
      "al13.371968269348145\n",
      "cl258.0815734863281\n",
      "total reward after 18 steps is 49.0 and avg reward is 25.789473684210527\n",
      "al13.0050048828125\n",
      "cl300.3663330078125\n",
      "total reward after 19 steps is 54.0 and avg reward is 27.2\n",
      "al13.810773849487305\n",
      "cl354.45782470703125\n",
      "total reward after 20 steps is 48.0 and avg reward is 28.19047619047619\n",
      "al11.738566398620605\n",
      "cl268.8330993652344\n",
      "total reward after 21 steps is 23.0 and avg reward is 27.954545454545453\n",
      "al2.6187849044799805\n",
      "cl31.74079704284668\n",
      "total reward after 22 steps is 32.0 and avg reward is 28.130434782608695\n",
      "al5.1433610916137695\n",
      "cl90.06451416015625\n",
      "total reward after 23 steps is 48.0 and avg reward is 28.958333333333332\n",
      "al11.094796180725098\n",
      "cl250.16656494140625\n",
      "total reward after 24 steps is 90.0 and avg reward is 31.4\n",
      "al23.865875244140625\n",
      "cl1057.26904296875\n",
      "total reward after 25 steps is 17.0 and avg reward is 30.846153846153847\n",
      "al-1.0246446132659912\n",
      "cl18.64387321472168\n",
      "total reward after 26 steps is 25.0 and avg reward is 30.62962962962963\n",
      "al4.056209564208984\n",
      "cl36.698787689208984\n",
      "total reward after 27 steps is 69.0 and avg reward is 32.0\n",
      "al16.313331604003906\n",
      "cl541.95458984375\n",
      "total reward after 28 steps is 32.0 and avg reward is 32.0\n",
      "al4.572042465209961\n",
      "cl59.06433868408203\n",
      "total reward after 29 steps is 29.0 and avg reward is 31.9\n",
      "al1.9066458940505981\n",
      "cl49.491737365722656\n",
      "total reward after 30 steps is 45.0 and avg reward is 32.32258064516129\n",
      "al6.777949810028076\n",
      "cl169.23367309570312\n",
      "total reward after 31 steps is 41.0 and avg reward is 32.59375\n",
      "al5.655925750732422\n",
      "cl119.87725067138672\n",
      "total reward after 32 steps is 65.0 and avg reward is 33.57575757575758\n",
      "al15.332818031311035\n",
      "cl452.7786865234375\n",
      "total reward after 33 steps is 44.0 and avg reward is 33.88235294117647\n",
      "al6.950286388397217\n",
      "cl138.9186248779297\n",
      "total reward after 34 steps is 44.0 and avg reward is 34.17142857142857\n",
      "al8.611797332763672\n",
      "cl170.45156860351562\n",
      "total reward after 35 steps is 45.0 and avg reward is 34.47222222222222\n",
      "al7.902329444885254\n",
      "cl156.85853576660156\n",
      "total reward after 36 steps is 69.0 and avg reward is 35.4054054054054\n",
      "al14.149703979492188\n",
      "cl456.8373107910156\n",
      "total reward after 37 steps is 81.0 and avg reward is 36.60526315789474\n",
      "al17.75588607788086\n",
      "cl665.2389526367188\n",
      "total reward after 38 steps is 68.0 and avg reward is 37.41025641025641\n",
      "al13.582659721374512\n",
      "cl423.4204406738281\n",
      "total reward after 39 steps is 112.0 and avg reward is 39.275\n",
      "al25.806621551513672\n",
      "cl1409.3829345703125\n",
      "total reward after 40 steps is 97.0 and avg reward is 40.68292682926829\n",
      "al20.651437759399414\n",
      "cl970.4400634765625\n",
      "total reward after 41 steps is 99.0 and avg reward is 42.07142857142857\n",
      "al20.736162185668945\n",
      "cl1002.538818359375\n",
      "total reward after 42 steps is 70.0 and avg reward is 42.72093023255814\n",
      "al13.70141315460205\n",
      "cl374.4477844238281\n",
      "total reward after 43 steps is 104.0 and avg reward is 44.11363636363637\n",
      "al21.50867462158203\n",
      "cl1068.6478271484375\n",
      "total reward after 44 steps is 85.0 and avg reward is 45.022222222222226\n",
      "al14.228388786315918\n",
      "cl600.96337890625\n",
      "total reward after 45 steps is 122.0 and avg reward is 46.69565217391305\n",
      "al25.149707794189453\n",
      "cl1523.5579833984375\n",
      "total reward after 46 steps is 95.0 and avg reward is 47.723404255319146\n",
      "al16.67462158203125\n",
      "cl805.3257446289062\n",
      "total reward after 47 steps is 114.0 and avg reward is 49.104166666666664\n",
      "al21.517162322998047\n",
      "cl1235.985595703125\n",
      "total reward after 48 steps is 114.0 and avg reward is 50.42857142857143\n",
      "al20.113134384155273\n",
      "cl1188.985595703125\n",
      "total reward after 49 steps is 118.0 and avg reward is 51.78\n",
      "al21.842077255249023\n",
      "cl1278.2552490234375\n",
      "total reward after 50 steps is 49.0 and avg reward is 51.72549019607843\n",
      "al1.055092215538025\n",
      "cl87.35426330566406\n",
      "total reward after 51 steps is 143.0 and avg reward is 53.48076923076923\n",
      "al26.25204849243164\n",
      "cl2027.079345703125\n",
      "total reward after 52 steps is 41.0 and avg reward is 53.24528301886792\n",
      "al-3.364926338195801\n",
      "cl92.98995208740234\n",
      "total reward after 53 steps is 58.0 and avg reward is 53.333333333333336\n",
      "al1.7600736618041992\n",
      "cl160.38143920898438\n",
      "total reward after 54 steps is 170.0 and avg reward is 55.45454545454545\n",
      "al31.82047462463379\n",
      "cl3061.410888671875\n",
      "total reward after 55 steps is 182.0 and avg reward is 57.714285714285715\n",
      "al35.33823013305664\n",
      "cl3553.99560546875\n",
      "total reward after 56 steps is 146.0 and avg reward is 59.26315789473684\n",
      "al22.68489646911621\n",
      "cl2016.862548828125\n",
      "total reward after 57 steps is 110.0 and avg reward is 60.13793103448276\n",
      "al12.780074119567871\n",
      "cl938.6962280273438\n",
      "total reward after 58 steps is 200.0 and avg reward is 62.50847457627118\n",
      "al40.394737243652344\n",
      "cl4195.6396484375\n",
      "total reward after 59 steps is 200.0 and avg reward is 64.8\n",
      "al37.40026092529297\n",
      "cl4098.35400390625\n",
      "total reward after 60 steps is 200.0 and avg reward is 67.01639344262296\n",
      "al36.73899459838867\n",
      "cl3946.00341796875\n",
      "total reward after 61 steps is 101.0 and avg reward is 67.56451612903226\n",
      "al7.56924295425415\n",
      "cl637.7496337890625\n",
      "total reward after 62 steps is 152.0 and avg reward is 68.9047619047619\n",
      "al18.525615692138672\n",
      "cl1896.4613037109375\n",
      "total reward after 63 steps is 103.0 and avg reward is 69.4375\n",
      "al5.095156669616699\n",
      "cl737.47021484375\n",
      "total reward after 64 steps is 64.0 and avg reward is 69.35384615384615\n",
      "al-0.8076344132423401\n",
      "cl243.29763793945312\n",
      "total reward after 65 steps is 159.0 and avg reward is 70.71212121212122\n",
      "al24.534690856933594\n",
      "cl2120.31689453125\n",
      "total reward after 66 steps is 200.0 and avg reward is 72.64179104477611\n",
      "al30.683115005493164\n",
      "cl3792.22021484375\n",
      "total reward after 67 steps is 200.0 and avg reward is 74.51470588235294\n",
      "al32.88129425048828\n",
      "cl3592.51220703125\n",
      "total reward after 68 steps is 200.0 and avg reward is 76.33333333333333\n",
      "al28.419496536254883\n",
      "cl3526.7568359375\n",
      "total reward after 69 steps is 200.0 and avg reward is 78.1\n",
      "al26.121387481689453\n",
      "cl3444.510986328125\n",
      "total reward after 70 steps is 136.0 and avg reward is 78.91549295774648\n",
      "al10.600128173828125\n",
      "cl1231.212890625\n",
      "total reward after 71 steps is 124.0 and avg reward is 79.54166666666667\n",
      "al6.78012752532959\n",
      "cl1025.927490234375\n",
      "total reward after 72 steps is 127.0 and avg reward is 80.1917808219178\n",
      "al8.272834777832031\n",
      "cl1045.043701171875\n",
      "total reward after 73 steps is 114.0 and avg reward is 80.64864864864865\n",
      "al4.633963584899902\n",
      "cl824.4536743164062\n",
      "total reward after 74 steps is 129.0 and avg reward is 81.29333333333334\n",
      "al11.192291259765625\n",
      "cl995.0028076171875\n",
      "total reward after 75 steps is 117.0 and avg reward is 81.76315789473684\n",
      "al6.370397567749023\n",
      "cl803.6981811523438\n",
      "total reward after 76 steps is 200.0 and avg reward is 83.2987012987013\n",
      "al28.701351165771484\n",
      "cl3280.7109375\n",
      "total reward after 77 steps is 180.0 and avg reward is 84.53846153846153\n",
      "al21.77177619934082\n",
      "cl2307.3349609375\n",
      "total reward after 78 steps is 200.0 and avg reward is 86.0\n",
      "al27.412723541259766\n",
      "cl3142.3671875\n",
      "total reward after 79 steps is 200.0 and avg reward is 87.425\n",
      "al26.647794723510742\n",
      "cl3086.49658203125\n",
      "total reward after 80 steps is 200.0 and avg reward is 88.81481481481481\n",
      "al27.9705810546875\n",
      "cl3223.6083984375\n",
      "total reward after 81 steps is 200.0 and avg reward is 90.17073170731707\n",
      "al27.58536720275879\n",
      "cl2934.765380859375\n",
      "total reward after 82 steps is 200.0 and avg reward is 91.49397590361446\n",
      "al21.31244468688965\n",
      "cl2825.313720703125\n",
      "total reward after 83 steps is 200.0 and avg reward is 92.78571428571429\n",
      "al23.764617919921875\n",
      "cl3028.30810546875\n",
      "total reward after 84 steps is 200.0 and avg reward is 94.04705882352941\n",
      "al23.60645866394043\n",
      "cl2757.593017578125\n",
      "total reward after 85 steps is 200.0 and avg reward is 95.27906976744185\n",
      "al19.348758697509766\n",
      "cl2733.705322265625\n",
      "total reward after 86 steps is 200.0 and avg reward is 96.48275862068965\n",
      "al23.682008743286133\n",
      "cl2704.921875\n",
      "total reward after 87 steps is 200.0 and avg reward is 97.6590909090909\n",
      "al21.430496215820312\n",
      "cl2642.225830078125\n",
      "total reward after 88 steps is 200.0 and avg reward is 98.80898876404494\n",
      "al19.371614456176758\n",
      "cl2779.968017578125\n",
      "total reward after 89 steps is 200.0 and avg reward is 99.93333333333334\n",
      "al19.88698959350586\n",
      "cl2532.684326171875\n",
      "total reward after 90 steps is 200.0 and avg reward is 101.03296703296704\n",
      "al21.638145446777344\n",
      "cl2838.92431640625\n",
      "total reward after 91 steps is 200.0 and avg reward is 102.1086956521739\n",
      "al19.161283493041992\n",
      "cl2608.265869140625\n",
      "total reward after 92 steps is 200.0 and avg reward is 103.16129032258064\n",
      "al18.214046478271484\n",
      "cl2469.8095703125\n",
      "total reward after 93 steps is 200.0 and avg reward is 104.19148936170212\n",
      "al14.759416580200195\n",
      "cl2671.25244140625\n",
      "total reward after 94 steps is 200.0 and avg reward is 105.2\n",
      "al16.995086669921875\n",
      "cl2471.3837890625\n",
      "total reward after 95 steps is 200.0 and avg reward is 106.1875\n",
      "al16.711589813232422\n",
      "cl2180.4765625\n",
      "total reward after 96 steps is 200.0 and avg reward is 107.15463917525773\n",
      "al12.704798698425293\n",
      "cl2191.6708984375\n",
      "total reward after 97 steps is 174.0 and avg reward is 107.83673469387755\n",
      "al7.147576808929443\n",
      "cl1752.490966796875\n",
      "total reward after 98 steps is 150.0 and avg reward is 108.26262626262626\n",
      "al-0.2105688452720642\n",
      "cl1300.09814453125\n",
      "total reward after 99 steps is 153.0 and avg reward is 108.71\n",
      "al2.8813281059265137\n",
      "cl1050.6070556640625\n",
      "total reward after 100 steps is 200.0 and avg reward is 110.47\n",
      "al17.05984878540039\n",
      "cl2428.362060546875\n",
      "total reward after 101 steps is 200.0 and avg reward is 112.29\n",
      "al17.850086212158203\n",
      "cl2167.242431640625\n",
      "total reward after 102 steps is 200.0 and avg reward is 113.99\n",
      "al16.778732299804688\n",
      "cl2158.662109375\n",
      "total reward after 103 steps is 200.0 and avg reward is 115.88\n",
      "al14.631413459777832\n",
      "cl2129.466552734375\n",
      "total reward after 104 steps is 200.0 and avg reward is 117.72\n",
      "al14.590158462524414\n",
      "cl2162.541015625\n",
      "total reward after 105 steps is 200.0 and avg reward is 119.59\n",
      "al11.637242317199707\n",
      "cl2063.7529296875\n",
      "total reward after 106 steps is 200.0 and avg reward is 121.31\n",
      "al13.890846252441406\n",
      "cl1985.4058837890625\n",
      "total reward after 107 steps is 200.0 and avg reward is 123.13\n",
      "al14.711509704589844\n",
      "cl1867.6612548828125\n",
      "total reward after 108 steps is 200.0 and avg reward is 124.84\n",
      "al11.54529857635498\n",
      "cl2196.67578125\n",
      "total reward after 109 steps is 200.0 and avg reward is 126.63\n",
      "al9.808287620544434\n",
      "cl2209.542236328125\n",
      "total reward after 110 steps is 200.0 and avg reward is 128.08\n",
      "al13.919437408447266\n",
      "cl2078.885009765625\n",
      "total reward after 111 steps is 129.0 and avg reward is 129.26\n",
      "al-5.6015543937683105\n",
      "cl510.34564208984375\n",
      "total reward after 112 steps is 125.0 and avg reward is 130.42\n",
      "al-1.1901074647903442\n",
      "cl373.87579345703125\n",
      "total reward after 113 steps is 200.0 and avg reward is 132.21\n",
      "al12.206563949584961\n",
      "cl1949.768310546875\n",
      "total reward after 114 steps is 200.0 and avg reward is 134.05\n",
      "al10.958049774169922\n",
      "cl2111.299560546875\n",
      "total reward after 115 steps is 200.0 and avg reward is 135.57\n",
      "al7.146472454071045\n",
      "cl1989.2220458984375\n",
      "total reward after 116 steps is 200.0 and avg reward is 137.29\n",
      "al8.581905364990234\n",
      "cl2128.314453125\n",
      "total reward after 117 steps is 200.0 and avg reward is 138.84\n",
      "al9.82370376586914\n",
      "cl2112.975830078125\n",
      "total reward after 118 steps is 200.0 and avg reward is 140.35\n",
      "al10.959875106811523\n",
      "cl1810.2744140625\n",
      "total reward after 119 steps is 200.0 and avg reward is 141.81\n",
      "al5.268782138824463\n",
      "cl1913.56396484375\n",
      "total reward after 120 steps is 200.0 and avg reward is 143.33\n",
      "al9.12217903137207\n",
      "cl1459.87109375\n",
      "total reward after 121 steps is 200.0 and avg reward is 145.1\n",
      "al6.307199478149414\n",
      "cl1478.0089111328125\n",
      "total reward after 122 steps is 200.0 and avg reward is 146.78\n",
      "al5.981223106384277\n",
      "cl1895.1685791015625\n",
      "total reward after 123 steps is 200.0 and avg reward is 148.3\n",
      "al6.304794788360596\n",
      "cl1519.685302734375\n",
      "total reward after 124 steps is 200.0 and avg reward is 149.4\n",
      "al1.058164119720459\n",
      "cl1897.116455078125\n",
      "total reward after 125 steps is 200.0 and avg reward is 151.23\n",
      "al5.737857341766357\n",
      "cl2062.524169921875\n",
      "total reward after 126 steps is 200.0 and avg reward is 152.98\n",
      "al6.4313435554504395\n",
      "cl1482.39013671875\n",
      "total reward after 127 steps is 200.0 and avg reward is 154.29\n",
      "al3.567440986633301\n",
      "cl1719.1904296875\n",
      "total reward after 128 steps is 200.0 and avg reward is 155.97\n",
      "al8.053003311157227\n",
      "cl1321.663330078125\n",
      "total reward after 129 steps is 200.0 and avg reward is 157.68\n",
      "al3.6221084594726562\n",
      "cl1351.3717041015625\n",
      "total reward after 130 steps is 200.0 and avg reward is 159.23\n",
      "al3.4552388191223145\n",
      "cl1476.8074951171875\n",
      "total reward after 131 steps is 200.0 and avg reward is 160.82\n",
      "al3.96224308013916\n",
      "cl1404.8717041015625\n",
      "total reward after 132 steps is 200.0 and avg reward is 162.17\n",
      "al1.7947962284088135\n",
      "cl1547.6561279296875\n",
      "total reward after 133 steps is 142.0 and avg reward is 163.15\n",
      "al-11.532242774963379\n",
      "cl951.3505859375\n",
      "total reward after 134 steps is 121.0 and avg reward is 163.92\n",
      "al-12.287073135375977\n",
      "cl1312.240966796875\n",
      "total reward after 135 steps is 127.0 and avg reward is 164.74\n",
      "al-14.872065544128418\n",
      "cl1410.270263671875\n",
      "total reward after 136 steps is 81.0 and avg reward is 164.86\n",
      "al-21.163774490356445\n",
      "cl1876.97216796875\n",
      "total reward after 137 steps is 123.0 and avg reward is 165.28\n",
      "al-9.94123649597168\n",
      "cl1039.8231201171875\n",
      "total reward after 138 steps is 200.0 and avg reward is 166.6\n",
      "al7.746487617492676\n",
      "cl1872.282958984375\n",
      "total reward after 139 steps is 158.0 and avg reward is 167.06\n",
      "al-0.8996431827545166\n",
      "cl988.4998168945312\n",
      "total reward after 140 steps is 200.0 and avg reward is 168.09\n",
      "al4.7318501472473145\n",
      "cl2011.8455810546875\n",
      "total reward after 141 steps is 200.0 and avg reward is 169.1\n",
      "al6.91774845123291\n",
      "cl1501.5703125\n",
      "total reward after 142 steps is 200.0 and avg reward is 170.4\n",
      "al2.9570682048797607\n",
      "cl2000.1055908203125\n",
      "total reward after 143 steps is 200.0 and avg reward is 171.36\n",
      "al5.307306289672852\n",
      "cl1836.0714111328125\n",
      "total reward after 144 steps is 200.0 and avg reward is 172.51\n",
      "al13.492490768432617\n",
      "cl1308.7188720703125\n",
      "total reward after 145 steps is 200.0 and avg reward is 173.29\n",
      "al3.4207262992858887\n",
      "cl1769.7091064453125\n",
      "total reward after 146 steps is 200.0 and avg reward is 174.34\n",
      "al0.2566183805465698\n",
      "cl1860.813720703125\n",
      "total reward after 147 steps is 200.0 and avg reward is 175.2\n",
      "al6.576610565185547\n",
      "cl1412.7030029296875\n",
      "total reward after 148 steps is 200.0 and avg reward is 176.06\n",
      "al2.708595037460327\n",
      "cl1726.3863525390625\n",
      "total reward after 149 steps is 200.0 and avg reward is 176.88\n",
      "al6.3715410232543945\n",
      "cl1398.3082275390625\n",
      "total reward after 150 steps is 200.0 and avg reward is 178.39\n",
      "al8.35487174987793\n",
      "cl1274.729248046875\n",
      "total reward after 151 steps is 200.0 and avg reward is 178.96\n",
      "al6.648631572723389\n",
      "cl1146.6368408203125\n",
      "total reward after 152 steps is 186.0 and avg reward is 180.41\n",
      "al1.3473678827285767\n",
      "cl1022.3474731445312\n",
      "total reward after 153 steps is 200.0 and avg reward is 181.83\n",
      "al-1.6278055906295776\n",
      "cl1832.9547119140625\n",
      "total reward after 154 steps is 200.0 and avg reward is 182.13\n",
      "al1.3419660329818726\n",
      "cl1704.5887451171875\n",
      "total reward after 155 steps is 200.0 and avg reward is 182.31\n",
      "al6.4624128341674805\n",
      "cl1249.7457275390625\n",
      "total reward after 156 steps is 200.0 and avg reward is 182.85\n",
      "al3.812122344970703\n",
      "cl1277.3077392578125\n",
      "total reward after 157 steps is 200.0 and avg reward is 183.75\n",
      "al5.175913333892822\n",
      "cl1152.1285400390625\n",
      "total reward after 158 steps is 200.0 and avg reward is 183.75\n",
      "al-0.12283656001091003\n",
      "cl1483.22265625\n",
      "total reward after 159 steps is 152.0 and avg reward is 183.27\n",
      "al-5.8302321434021\n",
      "cl561.60595703125\n",
      "total reward after 160 steps is 157.0 and avg reward is 182.84\n",
      "al-3.861919403076172\n",
      "cl669.6478271484375\n",
      "total reward after 161 steps is 200.0 and avg reward is 183.83\n",
      "al7.55999231338501\n",
      "cl1094.73291015625\n",
      "total reward after 162 steps is 200.0 and avg reward is 184.31\n",
      "al5.731121063232422\n",
      "cl1008.1797485351562\n",
      "total reward after 163 steps is 200.0 and avg reward is 185.28\n",
      "al4.10334587097168\n",
      "cl1343.6573486328125\n",
      "total reward after 164 steps is 200.0 and avg reward is 186.64\n",
      "al3.7793257236480713\n",
      "cl1420.474365234375\n",
      "total reward after 165 steps is 200.0 and avg reward is 187.05\n",
      "al-0.4989146888256073\n",
      "cl1453.838623046875\n",
      "total reward after 166 steps is 200.0 and avg reward is 187.05\n",
      "al0.09460600465536118\n",
      "cl1269.03662109375\n",
      "total reward after 167 steps is 200.0 and avg reward is 187.05\n",
      "al2.549372911453247\n",
      "cl1133.7020263671875\n",
      "total reward after 168 steps is 200.0 and avg reward is 187.05\n",
      "al1.4129279851913452\n",
      "cl1486.46240234375\n",
      "total reward after 169 steps is 200.0 and avg reward is 187.05\n",
      "al-8.910881996154785\n",
      "cl2544.316650390625\n",
      "total reward after 170 steps is 200.0 and avg reward is 187.69\n",
      "al-6.766748905181885\n",
      "cl2570.110107421875\n",
      "total reward after 171 steps is 200.0 and avg reward is 188.45\n",
      "al2.785398483276367\n",
      "cl1565.2020263671875\n",
      "total reward after 172 steps is 200.0 and avg reward is 189.18\n",
      "al-1.3498283624649048\n",
      "cl2012.5028076171875\n",
      "total reward after 173 steps is 200.0 and avg reward is 190.04\n",
      "al-9.77684497833252\n",
      "cl2382.207275390625\n",
      "total reward after 174 steps is 200.0 and avg reward is 190.75\n",
      "al2.0362091064453125\n",
      "cl1785.7437744140625\n",
      "total reward after 175 steps is 114.0 and avg reward is 190.72\n",
      "al-20.533912658691406\n",
      "cl1224.522216796875\n",
      "total reward after 176 steps is 200.0 and avg reward is 190.72\n",
      "al2.5258140563964844\n",
      "cl1929.65185546875\n",
      "total reward after 177 steps is 200.0 and avg reward is 190.92\n",
      "al1.695198893547058\n",
      "cl1638.141357421875\n",
      "total reward after 178 steps is 178.0 and avg reward is 190.7\n",
      "al-6.37339448928833\n",
      "cl1410.4239501953125\n",
      "total reward after 179 steps is 200.0 and avg reward is 190.7\n",
      "al1.0056904554367065\n",
      "cl1621.496826171875\n",
      "total reward after 180 steps is 200.0 and avg reward is 190.7\n",
      "al-2.376254081726074\n",
      "cl1808.748779296875\n",
      "total reward after 181 steps is 200.0 and avg reward is 190.7\n",
      "al0.7690438628196716\n",
      "cl1700.0067138671875\n",
      "total reward after 182 steps is 200.0 and avg reward is 190.7\n",
      "al-0.08233299106359482\n",
      "cl1572.6026611328125\n",
      "total reward after 183 steps is 172.0 and avg reward is 190.42\n",
      "al-6.305705547332764\n",
      "cl1079.7718505859375\n",
      "total reward after 184 steps is 200.0 and avg reward is 190.42\n",
      "al-1.737716555595398\n",
      "cl1709.95751953125\n",
      "total reward after 185 steps is 200.0 and avg reward is 190.42\n",
      "al4.707142353057861\n",
      "cl1636.843017578125\n",
      "total reward after 186 steps is 200.0 and avg reward is 190.42\n",
      "al-2.738954782485962\n",
      "cl1916.31982421875\n",
      "total reward after 187 steps is 200.0 and avg reward is 190.42\n",
      "al-2.4123635292053223\n",
      "cl2346.397705078125\n",
      "total reward after 188 steps is 200.0 and avg reward is 190.42\n",
      "al-2.2882299423217773\n",
      "cl1794.2027587890625\n",
      "total reward after 189 steps is 200.0 and avg reward is 190.42\n",
      "al3.8325023651123047\n",
      "cl1381.6451416015625\n",
      "total reward after 190 steps is 200.0 and avg reward is 190.42\n",
      "al-1.6482635736465454\n",
      "cl1685.545654296875\n",
      "total reward after 191 steps is 200.0 and avg reward is 190.42\n",
      "al-0.3362441658973694\n",
      "cl1869.07177734375\n",
      "total reward after 192 steps is 200.0 and avg reward is 190.42\n",
      "al-1.464950442314148\n",
      "cl1386.926513671875\n",
      "total reward after 193 steps is 200.0 and avg reward is 190.42\n",
      "al-1.9888880252838135\n",
      "cl1945.2493896484375\n",
      "total reward after 194 steps is 200.0 and avg reward is 190.42\n",
      "al-0.26834988594055176\n",
      "cl1717.5589599609375\n",
      "total reward after 195 steps is 200.0 and avg reward is 190.42\n",
      "al2.684356212615967\n",
      "cl1195.9996337890625\n",
      "total reward after 196 steps is 200.0 and avg reward is 190.42\n",
      "al-2.7750203609466553\n",
      "cl1900.35986328125\n",
      "total reward after 197 steps is 200.0 and avg reward is 190.68\n",
      "al1.0522305965423584\n",
      "cl1960.63427734375\n",
      "total reward after 198 steps is 200.0 and avg reward is 191.18\n",
      "al1.1858969926834106\n",
      "cl1404.22314453125\n",
      "total reward after 199 steps is 200.0 and avg reward is 191.65\n",
      "al1.5468367338180542\n",
      "cl1708.2249755859375\n",
      "total reward after 200 steps is 200.0 and avg reward is 191.65\n",
      "al0.2771628201007843\n",
      "cl1133.4656982421875\n",
      "total reward after 201 steps is 200.0 and avg reward is 191.65\n",
      "al-1.8288625478744507\n",
      "cl1795.9425048828125\n",
      "total reward after 202 steps is 200.0 and avg reward is 191.65\n",
      "al0.7274073362350464\n",
      "cl1461.0860595703125\n",
      "total reward after 203 steps is 200.0 and avg reward is 191.65\n",
      "al1.0197279453277588\n",
      "cl1637.2528076171875\n",
      "total reward after 204 steps is 200.0 and avg reward is 191.65\n",
      "al-0.2790006697177887\n",
      "cl1616.9666748046875\n",
      "total reward after 205 steps is 200.0 and avg reward is 191.65\n",
      "al2.369917154312134\n",
      "cl1392.4315185546875\n",
      "total reward after 206 steps is 200.0 and avg reward is 191.65\n",
      "al-0.4720776677131653\n",
      "cl1382.307861328125\n",
      "total reward after 207 steps is 200.0 and avg reward is 191.65\n",
      "al0.04317305609583855\n",
      "cl1557.64697265625\n",
      "total reward after 208 steps is 200.0 and avg reward is 191.65\n",
      "al-1.9176959991455078\n",
      "cl1459.9910888671875\n",
      "total reward after 209 steps is 151.0 and avg reward is 191.16\n",
      "al-11.938285827636719\n",
      "cl1666.8726806640625\n",
      "total reward after 210 steps is 200.0 and avg reward is 191.16\n",
      "al0.7985436320304871\n",
      "cl1395.951416015625\n",
      "total reward after 211 steps is 200.0 and avg reward is 191.87\n",
      "al1.247052788734436\n",
      "cl1354.218017578125\n",
      "total reward after 212 steps is 200.0 and avg reward is 192.62\n",
      "al6.588025093078613\n",
      "cl997.102783203125\n",
      "total reward after 213 steps is 200.0 and avg reward is 192.62\n",
      "al3.9409985542297363\n",
      "cl1004.3085327148438\n",
      "total reward after 214 steps is 200.0 and avg reward is 192.62\n",
      "al2.1431212425231934\n",
      "cl1222.989501953125\n",
      "total reward after 215 steps is 200.0 and avg reward is 192.62\n",
      "al-1.3248530626296997\n",
      "cl1279.9378662109375\n",
      "total reward after 216 steps is 200.0 and avg reward is 192.62\n",
      "al3.8404903411865234\n",
      "cl986.9195556640625\n",
      "total reward after 217 steps is 200.0 and avg reward is 192.62\n",
      "al2.3195607662200928\n",
      "cl1001.885498046875\n",
      "total reward after 218 steps is 200.0 and avg reward is 192.62\n",
      "al-1.2403440475463867\n",
      "cl1434.7811279296875\n",
      "total reward after 219 steps is 200.0 and avg reward is 192.62\n",
      "al3.0047554969787598\n",
      "cl891.6959228515625\n",
      "total reward after 220 steps is 200.0 and avg reward is 192.62\n",
      "al-4.411167621612549\n",
      "cl1254.3363037109375\n",
      "total reward after 221 steps is 200.0 and avg reward is 192.62\n",
      "al2.0628254413604736\n",
      "cl1024.8875732421875\n",
      "total reward after 222 steps is 200.0 and avg reward is 192.62\n",
      "al-1.6814868450164795\n",
      "cl1157.7825927734375\n",
      "total reward after 223 steps is 200.0 and avg reward is 192.62\n",
      "al2.1402435302734375\n",
      "cl1101.311767578125\n",
      "total reward after 224 steps is 200.0 and avg reward is 192.62\n",
      "al0.2513924241065979\n",
      "cl1221.86669921875\n",
      "total reward after 225 steps is 200.0 and avg reward is 192.62\n",
      "al-1.1239835023880005\n",
      "cl1255.4498291015625\n",
      "total reward after 226 steps is 200.0 and avg reward is 192.62\n",
      "al1.1939177513122559\n",
      "cl1125.6781005859375\n",
      "total reward after 227 steps is 200.0 and avg reward is 192.62\n",
      "al0.1822635531425476\n",
      "cl1270.114501953125\n",
      "total reward after 228 steps is 200.0 and avg reward is 192.62\n",
      "al0.7512245178222656\n",
      "cl994.0775756835938\n",
      "total reward after 229 steps is 200.0 and avg reward is 192.62\n",
      "al-1.870285153388977\n",
      "cl1144.0655517578125\n",
      "total reward after 230 steps is 200.0 and avg reward is 192.62\n",
      "al-2.1873607635498047\n",
      "cl1442.3822021484375\n",
      "total reward after 231 steps is 200.0 and avg reward is 192.62\n",
      "al2.0842971801757812\n",
      "cl940.0842895507812\n",
      "total reward after 232 steps is 200.0 and avg reward is 192.62\n",
      "al-2.5027713775634766\n",
      "cl1127.3509521484375\n",
      "total reward after 233 steps is 200.0 and avg reward is 193.2\n",
      "al5.586761951446533\n",
      "cl801.0154418945312\n",
      "total reward after 234 steps is 200.0 and avg reward is 193.99\n",
      "al1.296919345855713\n",
      "cl972.4043579101562\n",
      "total reward after 235 steps is 200.0 and avg reward is 194.72\n",
      "al1.1169112920761108\n",
      "cl910.98291015625\n",
      "total reward after 236 steps is 200.0 and avg reward is 195.91\n",
      "al0.43752461671829224\n",
      "cl921.8845825195312\n",
      "total reward after 237 steps is 200.0 and avg reward is 196.68\n",
      "al-1.3570475578308105\n",
      "cl908.7161865234375\n",
      "total reward after 238 steps is 200.0 and avg reward is 196.68\n",
      "al3.51446270942688\n",
      "cl599.8116455078125\n",
      "total reward after 239 steps is 195.0 and avg reward is 197.05\n",
      "al-0.7699428796768188\n",
      "cl622.1091918945312\n",
      "total reward after 240 steps is 200.0 and avg reward is 197.05\n",
      "al0.9576110243797302\n",
      "cl559.3372802734375\n",
      "total reward after 241 steps is 200.0 and avg reward is 197.05\n",
      "al2.3222620487213135\n",
      "cl733.5255737304688\n",
      "total reward after 242 steps is 200.0 and avg reward is 197.05\n",
      "al-0.9146501421928406\n",
      "cl1035.8917236328125\n",
      "total reward after 243 steps is 200.0 and avg reward is 197.05\n",
      "al-0.2852483093738556\n",
      "cl624.9472045898438\n",
      "total reward after 244 steps is 200.0 and avg reward is 197.05\n",
      "al4.718976020812988\n",
      "cl489.2879333496094\n",
      "total reward after 245 steps is 200.0 and avg reward is 197.05\n",
      "al5.449047565460205\n",
      "cl403.990478515625\n",
      "total reward after 246 steps is 200.0 and avg reward is 197.05\n",
      "al7.634111404418945\n",
      "cl471.6664733886719\n",
      "total reward after 247 steps is 200.0 and avg reward is 197.05\n",
      "al-2.8507721424102783\n",
      "cl599.1315307617188\n",
      "total reward after 248 steps is 152.0 and avg reward is 196.57\n",
      "al-10.618666648864746\n",
      "cl633.6659545898438\n",
      "total reward after 249 steps is 153.0 and avg reward is 196.1\n",
      "al-9.669017791748047\n",
      "cl315.0791320800781\n"
     ]
    }
   ],
   "source": [
    "def preprocess1(states, actions, rewards, gamma):\n",
    "    discnt_rewards = []\n",
    "    sum_reward = 0\n",
    "    rewards.reverse()\n",
    "    for r in rewards:\n",
    "      sum_reward = r + gamma*sum_reward\n",
    "      discnt_rewards.append(sum_reward)\n",
    "    discnt_rewards.reverse()\n",
    "    states = np.array(states, dtype=np.float32)\n",
    "    actions = np.array(actions, dtype=np.int32)\n",
    "    discnt_rewards = np.array(discnt_rewards, dtype=np.float32)\n",
    "\n",
    "    return states, actions, discnt_rewards\n",
    "\n",
    "tf.random.set_seed(336699)\n",
    "agentoo7 = agent()\n",
    "steps = 250\n",
    "ep_reward = []\n",
    "total_avgr = []\n",
    "for s in range(steps):\n",
    "  \n",
    "  done = False\n",
    "  state = env.reset()\n",
    "  total_reward = 0\n",
    "  all_aloss = []\n",
    "  all_closs = []\n",
    "  rewards = []\n",
    "  states = []\n",
    "  actions = []\n",
    "  \n",
    "  while not done:\n",
    "  \n",
    "    action = agentoo7.act(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    states.append(state)\n",
    "    #actions.append(tf.one_hot(action, 2, dtype=tf.int32).numpy().tolist())\n",
    "    actions.append(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        ep_reward.append(total_reward)\n",
    "        avg_reward = np.mean(ep_reward[-100:])\n",
    "        total_avgr.append(avg_reward)\n",
    "        print(\"total reward after {} steps is {} and avg reward is {}\".format(s, total_reward, avg_reward))\n",
    "        states, actions, discnt_rewards = preprocess1(states, actions, rewards, 1)\n",
    "  \n",
    "        al,cl = agentoo7.learn(states, actions, discnt_rewards) \n",
    "        print(f\"al{al}\") \n",
    "        print(f\"cl{cl}\")      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fFe9N8TxGVkR"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "pquOE2v0Snsh",
    "outputId": "4862be43-49d5-4799-d43c-ddfd0247f5a6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ep = [i  for i in range(250)]\n",
    "plt.plot(ep,total_avgr,'b')\n",
    "plt.title(\"avg reward Vs episods\")\n",
    "plt.xlabel(\"episods\")\n",
    "plt.ylabel(\"average reward per 100 episods\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ECJEQ7yJilE4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPadLJMBnxBnr+civM/20rL",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "A2C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
