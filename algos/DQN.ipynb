{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNPRdMK9P/GIEjvtu5dIQE4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhisheksuran/Atari_DQN/blob/master/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgCg9fghYF5N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ea79de95-9c77-40f2-912d-127ff8be8b51"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "from tensorflow.keras.models import load_model\n",
        "!pip3 install box2d-py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\r\u001b[K     |▊                               | 10kB 19.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 4.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 6.0MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 6.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 7.1MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRFJPH3QJNF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"LunarLander-v2\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPS71gZrYaw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class model(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.d1 = tf.keras.layers.Dense(256,activation='relu')\n",
        "    self.d2 = tf.keras.layers.Dense(256,activation='relu')\n",
        "    self.out = tf.keras.layers.Dense(4,activation=None)\n",
        "\n",
        "  def call(self, input_data):\n",
        "    x = self.d1(input_data)\n",
        "    x = self.d2(x)\n",
        "    x = self.out(x)\n",
        "    return x\n",
        "\n",
        "  def action_value(self, state):\n",
        "        q_values = self.predict(state)\n",
        "        return q_values"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gpDPUJLD3lG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class experience():\n",
        "  def __init__(self, buffer_size, state_dim):\n",
        "    self.buffer_size = buffer_size\n",
        "    self.pointer = 0\n",
        "    self.state_mem = np.zeros((self.buffer_size, *state_dim), dtype=np.float32)\n",
        "    self.action_mem = np.zeros(self.buffer_size, dtype=np.int32)\n",
        "    self.next_state_mem = np.zeros((self.buffer_size, *state_dim), dtype=np.float32)\n",
        "    self.reward_mem = np.zeros(self.buffer_size, dtype=np.int32)\n",
        "    self.done_mem = np.zeros(self.buffer_size, dtype=np.bool)\n",
        "\n",
        "  def add_exp(self, state, action, reward, next_state, done):\n",
        "    idx = self.pointer % self.buffer_size\n",
        "    self.state_mem[idx] = state\n",
        "    self.action_mem[idx] = action\n",
        "    self.reward_mem[idx] = reward\n",
        "    self.next_state_mem[idx] = next_state\n",
        "    self.done_mem[idx] = done\n",
        "    self.pointer += 1\n",
        "\n",
        "  def sample_exp(self, batch_size):\n",
        "    max_mem = min(self.pointer, self.buffer_size)\n",
        "    batch = np.random.choice(max_mem, batch_size, replace= False)\n",
        "    state = self.state_mem[batch]\n",
        "    action = self.action_mem[batch]\n",
        "    reward = self.reward_mem[batch]\n",
        "    next_state = self.next_state_mem[batch]\n",
        "    done = self.done_mem[batch]\n",
        "    return state, action , reward, next_state, done"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymK5zzMNEmFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class agent():\n",
        "  def __init__(self):\n",
        "    self.q_net = model()\n",
        "    self.target_net = model()\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    self.q_net.compile(optimizer=opt, loss='mse')\n",
        "    self.target_net.compile(optimizer=opt, loss='mse')\n",
        "    self.epsilon = 1.0\n",
        "    self.epsilon_decay = 1e-3\n",
        "    self.min_epsilon = 0.01\n",
        "    self.memory = experience(buffer_size=1000000, state_dim=env.observation_space.shape)\n",
        "    self.batch_size = 64\n",
        "    self.gamma = 0.99\n",
        "    self.replace = 100\n",
        "    self.trainstep = 0\n",
        "    self.action_space = [i for i in range(4)]\n",
        "  def act(self, state):\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "      #action = env.action_space.sample()\n",
        "      action = np.random.choice(self.action_space)\n",
        "\n",
        "    else:\n",
        "      state = np.array([state])\n",
        "      action = self.q_net.action_value(state)\n",
        "      action = np.argmax(action)\n",
        "\n",
        "    return action\n",
        "\n",
        "  def train(self):\n",
        "      if self.memory.pointer < self.batch_size:\n",
        "        return \n",
        "\n",
        "      if self.trainstep % self.replace == 0:\n",
        "        self.update_target()\n",
        "\n",
        "      states, actions, rewards, next_states, dones = self.memory.sample_exp(self.batch_size)\n",
        "      target = self.q_net.action_value(states)\n",
        "      next_state_val = self.target_net.action_value(next_states)\n",
        "      q_next = tf.math.reduce_max(next_state_val, axis=1, keepdims=True).numpy()\n",
        "      #print(\"next state pred {}\".format(next_state_val))\n",
        "      q_target = np.copy(target)\n",
        "      for i, d in enumerate(dones):\n",
        "        if d:\n",
        "          q_target[i, actions[i]] = rewards[i]\n",
        "\n",
        "        else:\n",
        "          #q_target[i, actions[i]] = rewards[i] + self.gamma * np.max(next_state_val[i])\n",
        "          q_target[i, actions[i]] = rewards[i] + self.gamma * q_next[i]\n",
        "\n",
        "      #print(states)\n",
        "      #print(q_target)    \n",
        "      self.q_net.train_on_batch(states, q_target)\n",
        "      self.update_epsilon()  \n",
        "      self.trainstep +=1\n",
        "\n",
        "\n",
        "  def update_mem(self, state, action, reward, next_state, done):\n",
        "    self.memory.add_exp(state, action, reward, next_state, done)\n",
        "\n",
        "\n",
        "  def update_target(self):\n",
        "    self.target_net.set_weights(self.q_net.get_weights())  \n",
        "\n",
        "\n",
        "  def update_epsilon(self):\n",
        "    self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon > self.min_epsilon else self.min_epsilon\n",
        "    return self.epsilon\n",
        "\n",
        "\n",
        "\n",
        "  def save_model(self):\n",
        "      self.q_net.save(\"model.h5\")\n",
        "      self.target_net.save(\"target_model.h5\")\n",
        "\n",
        "\n",
        "  def load_model(self):\n",
        "        self.q_net = load_model(\"model.h5\")\n",
        "        self.target_net = load_model(\"model.h5\")\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZYwk3ksITw-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1119f288-c1bb-4a35-c084-7baea5fc2697"
      },
      "source": [
        "agentoo7 = agent()\n",
        "steps = 500\n",
        "\n",
        "for s in range(steps):\n",
        "  done = False\n",
        "  state = env.reset()\n",
        "  total_reward = 0\n",
        "  t = 0\n",
        "  while not done:\n",
        "    #env.render()\n",
        "    action = agentoo7.act(state)\n",
        "    #print(action)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    agentoo7.update_mem(state, action, reward, next_state, done)\n",
        "    agentoo7.train()\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "    t += 1\n",
        "    if done:\n",
        "       print(\"total reward after {} episode is {} and epsilon is {}\".format(s, total_reward, agentoo7.epsilon))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total reward after 0 episode is -42.61811453267964 and epsilon is 0.951\n",
            "total reward after 1 episode is -410.4795281853554 and epsilon is 0.8649999999999999\n",
            "total reward after 2 episode is -445.9066218745269 and epsilon is 0.7139999999999997\n",
            "total reward after 3 episode is -142.0928661719887 and epsilon is 0.6159999999999997\n",
            "total reward after 4 episode is -267.5745308339568 and epsilon is 0.4539999999999995\n",
            "total reward after 5 episode is -305.9556669046533 and epsilon is 0.26899999999999935\n",
            "total reward after 6 episode is -175.2796966257096 and epsilon is 0.14999999999999925\n",
            "total reward after 7 episode is -278.22697309162163 and epsilon is 0.0929999999999992\n",
            "total reward after 8 episode is -196.788924444431 and epsilon is 0.01\n",
            "total reward after 9 episode is -70.10930961587783 and epsilon is 0.01\n",
            "total reward after 10 episode is -66.53212194089366 and epsilon is 0.01\n",
            "total reward after 11 episode is -87.47889383235864 and epsilon is 0.01\n",
            "total reward after 12 episode is -47.56540055268495 and epsilon is 0.01\n",
            "total reward after 13 episode is -25.81326898604968 and epsilon is 0.01\n",
            "total reward after 14 episode is -23.247034477076113 and epsilon is 0.01\n",
            "total reward after 15 episode is -20.644496629416423 and epsilon is 0.01\n",
            "total reward after 16 episode is 10.026043347458298 and epsilon is 0.01\n",
            "total reward after 17 episode is 11.734426592976106 and epsilon is 0.01\n",
            "total reward after 18 episode is -103.34386376779217 and epsilon is 0.01\n",
            "total reward after 19 episode is 13.72016065695783 and epsilon is 0.01\n",
            "total reward after 20 episode is -62.66481545917064 and epsilon is 0.01\n",
            "total reward after 21 episode is -45.70569972314351 and epsilon is 0.01\n",
            "total reward after 22 episode is 144.28541148851684 and epsilon is 0.01\n",
            "total reward after 23 episode is -84.49935522063859 and epsilon is 0.01\n",
            "total reward after 24 episode is -70.48984907206658 and epsilon is 0.01\n",
            "total reward after 25 episode is 1.683345105365646 and epsilon is 0.01\n",
            "total reward after 26 episode is -55.78828679365429 and epsilon is 0.01\n",
            "total reward after 27 episode is -27.847954701950478 and epsilon is 0.01\n",
            "total reward after 28 episode is -62.003908488132495 and epsilon is 0.01\n",
            "total reward after 29 episode is -133.17284463697342 and epsilon is 0.01\n",
            "total reward after 30 episode is -71.41077925031243 and epsilon is 0.01\n",
            "total reward after 31 episode is -45.70475812509928 and epsilon is 0.01\n",
            "total reward after 32 episode is -31.94750979846196 and epsilon is 0.01\n",
            "total reward after 33 episode is -71.59742725719136 and epsilon is 0.01\n",
            "total reward after 34 episode is -52.98104694780668 and epsilon is 0.01\n",
            "total reward after 35 episode is -28.882600931680958 and epsilon is 0.01\n",
            "total reward after 36 episode is -258.29974788611287 and epsilon is 0.01\n",
            "total reward after 37 episode is -66.36761466418196 and epsilon is 0.01\n",
            "total reward after 38 episode is -56.854601727055936 and epsilon is 0.01\n",
            "total reward after 39 episode is -107.05413393501428 and epsilon is 0.01\n",
            "total reward after 40 episode is -616.6923113491287 and epsilon is 0.01\n",
            "total reward after 41 episode is -37.31094463244169 and epsilon is 0.01\n",
            "total reward after 42 episode is -5.101427521615962 and epsilon is 0.01\n",
            "total reward after 43 episode is -53.625869977481464 and epsilon is 0.01\n",
            "total reward after 44 episode is -40.011648928078245 and epsilon is 0.01\n",
            "total reward after 45 episode is 24.8816203022604 and epsilon is 0.01\n",
            "total reward after 46 episode is -65.25011916926479 and epsilon is 0.01\n",
            "total reward after 47 episode is -30.559943944850076 and epsilon is 0.01\n",
            "total reward after 48 episode is -125.16841933030804 and epsilon is 0.01\n",
            "total reward after 49 episode is -36.72584983412973 and epsilon is 0.01\n",
            "total reward after 50 episode is -98.8176159547233 and epsilon is 0.01\n",
            "total reward after 51 episode is -13.271551724520762 and epsilon is 0.01\n",
            "total reward after 52 episode is -61.82927558815114 and epsilon is 0.01\n",
            "total reward after 53 episode is -136.3872160991753 and epsilon is 0.01\n",
            "total reward after 54 episode is 17.081263235641316 and epsilon is 0.01\n",
            "total reward after 55 episode is -66.7716716574032 and epsilon is 0.01\n",
            "total reward after 56 episode is -67.5615725575691 and epsilon is 0.01\n",
            "total reward after 57 episode is -12.030667242798609 and epsilon is 0.01\n",
            "total reward after 58 episode is -39.9477845262905 and epsilon is 0.01\n",
            "total reward after 59 episode is -46.622156797864974 and epsilon is 0.01\n",
            "total reward after 60 episode is 16.307736939854607 and epsilon is 0.01\n",
            "total reward after 61 episode is -284.08414144908727 and epsilon is 0.01\n",
            "total reward after 62 episode is -57.06927633617892 and epsilon is 0.01\n",
            "total reward after 63 episode is -67.39354529501968 and epsilon is 0.01\n",
            "total reward after 64 episode is -7.136033869996055 and epsilon is 0.01\n",
            "total reward after 65 episode is 26.047901778672276 and epsilon is 0.01\n",
            "total reward after 66 episode is 201.95567268332428 and epsilon is 0.01\n",
            "total reward after 67 episode is -37.09399179517604 and epsilon is 0.01\n",
            "total reward after 68 episode is 203.47785947261943 and epsilon is 0.01\n",
            "total reward after 69 episode is -78.44824051684162 and epsilon is 0.01\n",
            "total reward after 70 episode is 267.3534712302054 and epsilon is 0.01\n",
            "total reward after 71 episode is 211.81560452851886 and epsilon is 0.01\n",
            "total reward after 72 episode is 68.4642078916544 and epsilon is 0.01\n",
            "total reward after 73 episode is 233.9537653164513 and epsilon is 0.01\n",
            "total reward after 74 episode is 180.73841097449747 and epsilon is 0.01\n",
            "total reward after 75 episode is 231.24831496380463 and epsilon is 0.01\n",
            "total reward after 76 episode is 230.27155270795632 and epsilon is 0.01\n",
            "total reward after 77 episode is 86.70378195159883 and epsilon is 0.01\n",
            "total reward after 78 episode is 214.96888189509727 and epsilon is 0.01\n",
            "total reward after 79 episode is 260.09699573454986 and epsilon is 0.01\n",
            "total reward after 80 episode is 229.19633518773887 and epsilon is 0.01\n",
            "total reward after 81 episode is 275.46958117048996 and epsilon is 0.01\n",
            "total reward after 82 episode is 235.1416563472527 and epsilon is 0.01\n",
            "total reward after 83 episode is 250.60968730632453 and epsilon is 0.01\n",
            "total reward after 84 episode is 255.77635204185182 and epsilon is 0.01\n",
            "total reward after 85 episode is 225.38936958692784 and epsilon is 0.01\n",
            "total reward after 86 episode is 209.14798752574148 and epsilon is 0.01\n",
            "total reward after 87 episode is 236.12433099630994 and epsilon is 0.01\n",
            "total reward after 88 episode is 247.56439351658292 and epsilon is 0.01\n",
            "total reward after 89 episode is 232.62536143051673 and epsilon is 0.01\n",
            "total reward after 90 episode is 154.45599784606912 and epsilon is 0.01\n",
            "total reward after 91 episode is 182.6769459659725 and epsilon is 0.01\n",
            "total reward after 92 episode is 233.739563889219 and epsilon is 0.01\n",
            "total reward after 93 episode is 251.43894476710952 and epsilon is 0.01\n",
            "total reward after 94 episode is 253.73360567569767 and epsilon is 0.01\n",
            "total reward after 95 episode is -41.44329036339607 and epsilon is 0.01\n",
            "total reward after 96 episode is 169.3183805753841 and epsilon is 0.01\n",
            "total reward after 97 episode is 236.86380527133403 and epsilon is 0.01\n",
            "total reward after 98 episode is -71.22877643350719 and epsilon is 0.01\n",
            "total reward after 99 episode is 183.90686520487682 and epsilon is 0.01\n",
            "total reward after 100 episode is -297.4117167545884 and epsilon is 0.01\n",
            "total reward after 101 episode is -9.201354553601155 and epsilon is 0.01\n",
            "total reward after 102 episode is 46.59121688157224 and epsilon is 0.01\n",
            "total reward after 103 episode is 208.3406308431226 and epsilon is 0.01\n",
            "total reward after 104 episode is 184.20748059582763 and epsilon is 0.01\n",
            "total reward after 105 episode is -119.0164730216312 and epsilon is 0.01\n",
            "total reward after 106 episode is 211.05578434890168 and epsilon is 0.01\n",
            "total reward after 107 episode is -79.79604434334017 and epsilon is 0.01\n",
            "total reward after 108 episode is 251.15422132569034 and epsilon is 0.01\n",
            "total reward after 109 episode is -29.764981928172674 and epsilon is 0.01\n",
            "total reward after 110 episode is 243.8272099498237 and epsilon is 0.01\n",
            "total reward after 111 episode is -92.2573134508052 and epsilon is 0.01\n",
            "total reward after 112 episode is 212.2391526483663 and epsilon is 0.01\n",
            "total reward after 113 episode is 251.30401071235195 and epsilon is 0.01\n",
            "total reward after 114 episode is -276.86969874186474 and epsilon is 0.01\n",
            "total reward after 115 episode is -216.09624504383805 and epsilon is 0.01\n",
            "total reward after 116 episode is 196.17607129311892 and epsilon is 0.01\n",
            "total reward after 117 episode is 202.94801459606822 and epsilon is 0.01\n",
            "total reward after 118 episode is 209.83978734198632 and epsilon is 0.01\n",
            "total reward after 119 episode is -173.12927128645424 and epsilon is 0.01\n",
            "total reward after 120 episode is -57.184839774731316 and epsilon is 0.01\n",
            "total reward after 121 episode is 192.6241975766782 and epsilon is 0.01\n",
            "total reward after 122 episode is 187.5618497335122 and epsilon is 0.01\n",
            "total reward after 123 episode is 134.39636163858975 and epsilon is 0.01\n",
            "total reward after 124 episode is 154.4473113856155 and epsilon is 0.01\n",
            "total reward after 125 episode is 234.46450616415268 and epsilon is 0.01\n",
            "total reward after 126 episode is 235.8008677779049 and epsilon is 0.01\n",
            "total reward after 127 episode is 266.74413515856725 and epsilon is 0.01\n",
            "total reward after 128 episode is 259.318809028858 and epsilon is 0.01\n",
            "total reward after 129 episode is 272.8696152798682 and epsilon is 0.01\n",
            "total reward after 130 episode is 251.1726359281042 and epsilon is 0.01\n",
            "total reward after 131 episode is 250.4019308903048 and epsilon is 0.01\n",
            "total reward after 132 episode is 274.67457158423525 and epsilon is 0.01\n",
            "total reward after 133 episode is 229.82947265292796 and epsilon is 0.01\n",
            "total reward after 134 episode is 302.5770447139121 and epsilon is 0.01\n",
            "total reward after 135 episode is 217.47801013479938 and epsilon is 0.01\n",
            "total reward after 136 episode is 239.8832204493655 and epsilon is 0.01\n",
            "total reward after 137 episode is 280.53711974060184 and epsilon is 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTYYAr79I-AT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}