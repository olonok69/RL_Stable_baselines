{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SoftACwithtau.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7kofcu1DW2WMxTUXkie2c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhisheksuran/Reinforcement_Learning/blob/master/SoftACwithtau.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQQaz7ifBP3Z",
        "outputId": "c70f1f7a-805c-462c-d452-0747d820f459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow_probability as tfp\n",
        "!pip3 install box2d-py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\r\u001b[K     |▊                               | 10kB 8.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 1.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 1.7MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 2.6MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efKIDfYGBSUi",
        "outputId": "f42af3e9-6f3f-4388-efe4-d5b76b38cee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "print(tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "env= gym.make(\"LunarLanderContinuous-v2\")\n",
        "state_low = env.observation_space.low\n",
        "state_high = env.observation_space.high\n",
        "action_low = env.action_space.low \n",
        "action_high = env.action_space.high\n",
        "print(state_low)\n",
        "print(state_high)\n",
        "print(action_low)\n",
        "print(action_high)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "[-inf -inf -inf -inf -inf -inf -inf -inf]\n",
            "[inf inf inf inf inf inf inf inf]\n",
            "[-1. -1.]\n",
            "[1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SwrILlcQpii"
      },
      "source": [
        "class RBuffer():\n",
        "  def __init__(self, maxsize, statedim, naction):\n",
        "    self.cnt = 0\n",
        "    self.maxsize = maxsize\n",
        "    self.state_memory = np.zeros((maxsize, *statedim), dtype=np.float32)\n",
        "    self.action_memory = np.zeros((maxsize, naction), dtype=np.float32)\n",
        "    self.reward_memory = np.zeros((maxsize,), dtype=np.float32)\n",
        "    self.next_state_memory = np.zeros((maxsize, *statedim), dtype=np.float32)\n",
        "    self.done_memory = np.zeros((maxsize,), dtype= np.bool)\n",
        "\n",
        "  def storexp(self, state, next_state, action, done, reward):\n",
        "    index = self.cnt % self.maxsize\n",
        "    self.state_memory[index] = state\n",
        "    self.action_memory[index] = action\n",
        "    self.reward_memory[index] = reward\n",
        "    self.next_state_memory[index] = next_state\n",
        "    self.done_memory[index] = 1- int(done)\n",
        "    self.cnt += 1\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    max_mem = min(self.cnt, self.maxsize)\n",
        "    batch = np.random.choice(max_mem, batch_size, replace= False)  \n",
        "    states = self.state_memory[batch]\n",
        "    next_states = self.next_state_memory[batch]\n",
        "    rewards = self.reward_memory[batch]\n",
        "    actions = self.action_memory[batch]\n",
        "    dones = self.done_memory[batch]\n",
        "    return states, next_states, rewards, actions, dones\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA4t0oizCM0P",
        "outputId": "dfe6f122-0768-4d1d-aa48-1c08eda90680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "class Critic(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Critic, self).__init__()\n",
        "    self.f1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.f2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.q =  tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "  def call(self, inputstate, action):\n",
        "    x = self.f1(tf.concat([inputstate, action], axis=1))\n",
        "    x = self.f2(x)\n",
        "    x = self.q(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class value_net(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(value_net, self).__init__()\n",
        "    self.f1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.f2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.v =  tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "  def call(self, inputstate):\n",
        "    x = self.f1(inputstate)\n",
        "    x = self.f2(x)\n",
        "    x = self.v(x)\n",
        "    return x\n",
        "    \n",
        "\n",
        "class Actor(tf.keras.Model):\n",
        "  def __init__(self, no_action):\n",
        "    super(Actor, self).__init__()    \n",
        "    self.f1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.f2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.mu =  tf.keras.layers.Dense(no_action, activation=None)\n",
        "    self.sigma =  tf.keras.layers.Dense(no_action, activation=None)\n",
        "    self.min_action = -1\n",
        "    self.max_action = 1\n",
        "    self.repram = 1e-6\n",
        "\n",
        "  def call(self, state):\n",
        "    x = self.f1(state)\n",
        "    x = self.f2(x)\n",
        "    mu = self.mu(x)\n",
        "    s = self.sigma(x)\n",
        "    s = tf.clip_by_value(s, self.repram, 1)\n",
        "    return mu, s\n",
        "    \n",
        "  def sample_normal(self, state, reparameterize=True):\n",
        "    mu, sigma = self(state)\n",
        "    # print(mu)\n",
        "    # print(sigma)\n",
        "    #mu = tf.squeeze(mu)\n",
        "    #sigma =tf.squeeze(sigma)\n",
        "    #print(mu)\n",
        "    probabilities = tfp.distributions.Normal(mu, sigma)\n",
        "    if reparameterize:\n",
        "        actions = probabilities.sample()\n",
        "        #actions += tf.random.normal(shape=tf.shape(actions), mean=0.0, stddev=0.1)\n",
        "        \n",
        "    else:\n",
        "        actions = probabilities.sample()\n",
        "        \n",
        "    action = tf.math.scalar_mul(tf.constant(self.max_action, dtype=tf.float32),tf.math.tanh(actions))\n",
        "    action = tf.squeeze(action)\n",
        "    log_prob = probabilities.log_prob(actions)\n",
        "    log_prob -= tf.math.log(1 - tf.math.pow(action, 2) + self.repram)\n",
        "    log_prob = tf.reduce_sum(log_prob, axis=1)\n",
        "    \n",
        "    return action, log_prob\n",
        "        \n",
        "\n",
        "\n",
        "class Agent():\n",
        "  def __init__(self, n_action= len(env.action_space.high)):\n",
        "    self.actor_main = Actor(n_action)\n",
        "    self.critic_1 = Critic()\n",
        "    self.critic_2 = Critic()\n",
        "    self.value_net = value_net()\n",
        "    self.target_value_net = value_net()\n",
        "    self.batch_size = 64\n",
        "    self.n_actions = len(env.action_space.high)\n",
        "    self.a_opt = tf.keras.optimizers.Adam(0.001)\n",
        "    # self.actor_target = tf.keras.optimizers.Adam(.001)\n",
        "    self.c_opt1 = tf.keras.optimizers.Adam(0.002)\n",
        "    self.c_opt2 = tf.keras.optimizers.Adam(0.002)\n",
        "    self.v_opt = tf.keras.optimizers.Adam(0.002)\n",
        "    # self.critic_target = tf.keras.optimizers.Adam(.002)\n",
        "    self.memory = RBuffer(1_00_000, env.observation_space.shape, len(env.action_space.high))\n",
        "    self.trainstep = 0\n",
        "    #self.replace = 5\n",
        "    self.gamma = 0.99\n",
        "    self.min_action = env.action_space.low[0]\n",
        "    self.max_action = env.action_space.high[0]\n",
        "    self.scale = 2\n",
        "    self.tau = 0.005\n",
        "    self.value_net.compile(optimizer=self.v_opt)\n",
        "    \n",
        "\n",
        "  def act(self, state):\n",
        "      state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "      action, _ = self.actor_main.sample_normal(state, reparameterize=False)\n",
        "      # print(action)\n",
        "      return action\n",
        "\n",
        "\n",
        "  def savexp(self,state, next_state, action, done, reward):\n",
        "        self.memory.storexp(state, next_state, action, done, reward)\n",
        "\n",
        "\n",
        "  def update_target(self, tau=None):\n",
        "  \n",
        "    if tau is None:\n",
        "        tau = self.tau\n",
        "\n",
        "    weights1 = []\n",
        "    targets1 = self.target_value_net.weights\n",
        "    for i, weight in enumerate(self.value_net.weights):\n",
        "        weights1.append(weight * tau + targets1[i]*(1-tau))\n",
        "    self.target_value_net.set_weights(weights1)\n",
        "\n",
        "  \n",
        "  def train(self):\n",
        "      if self.memory.cnt < self.batch_size:\n",
        "        return \n",
        "\n",
        "\n",
        "      states, next_states, rewards, actions, dones = self.memory.sample(self.batch_size)\n",
        "  \n",
        "      states = tf.convert_to_tensor(states, dtype= tf.float32)\n",
        "      next_states = tf.convert_to_tensor(next_states, dtype= tf.float32)\n",
        "      rewards = tf.convert_to_tensor(rewards, dtype= tf.float32)\n",
        "      actions = tf.convert_to_tensor(actions, dtype= tf.float32)\n",
        "      #dones = tf.convert_to_tensor(dones, dtype= tf.bool)\n",
        "      \n",
        "      \n",
        "      with tf.GradientTape() as tape1, tf.GradientTape() as tape2, tf.GradientTape() as tape3, tf.GradientTape() as tape4:\n",
        "          value = tf.squeeze(self.value_net(states))\n",
        "          #value = self.value_net(states)\n",
        "          #value loss\n",
        "          v_actions, v_log_probs = self.actor_main.sample_normal(states, reparameterize=False)\n",
        "          #print(v_log_probs)\n",
        "          #v_log_probs = tf.squeeze(v_log_probs, 1)\n",
        "          v_q1 = self.critic_1(states, v_actions)\n",
        "          v_q2 = self.critic_2(states, v_actions)\n",
        "          v_critic_value = tf.math.minimum(tf.squeeze(v_q1), tf.squeeze(v_q2))\n",
        "          target_value = v_critic_value - v_log_probs\n",
        "          #print(target_value)   \n",
        "          value_loss = 0.5 * tf.keras.losses.MSE(target_value, value)\n",
        "      \n",
        "          #actor loss\n",
        "          a_actions, a_log_probs = self.actor_main.sample_normal(states, reparameterize=True)\n",
        "          #a_log_probs = tf.squeeze(a_log_probs, 1)\n",
        "          a_q1 = self.critic_1(states, a_actions)\n",
        "          a_q2 = self.critic_2(states, a_actions)\n",
        "          a_critic_value = tf.math.minimum(tf.squeeze(a_q1), tf.squeeze(a_q2))\n",
        "          actor_loss =  a_log_probs - a_critic_value\n",
        "          actor_loss = tf.reduce_mean(actor_loss)\n",
        "    \n",
        "          next_state_value = tf.squeeze(self.target_value_net(next_states)) \n",
        "          #next_state_value = self.target_value_net(next_states)\n",
        "          #critic loss          \n",
        "          q_hat = self.scale * rewards + self.gamma * next_state_value * dones\n",
        "          c_q1 = self.critic_1(states, actions)\n",
        "          c_q2 = self.critic_2(states, actions)\n",
        "          critic_loss1 = 0.5 * tf.keras.losses.MSE(q_hat, tf.squeeze(c_q1))\n",
        "          critic_loss2 = 0.5 * tf.keras.losses.MSE(q_hat, tf.squeeze(c_q2))  \n",
        "      \n",
        "      grads1 = tape1.gradient(value_loss, self.value_net.trainable_variables)\n",
        "      grads2 = tape2.gradient(actor_loss, self.actor_main.trainable_variables)\n",
        "      grads3 = tape3.gradient(critic_loss1, self.critic_1.trainable_variables)\n",
        "      grads4 = tape4.gradient(critic_loss2, self.critic_2.trainable_variables)\n",
        "      self.v_opt.apply_gradients(zip(grads1, self.value_net.trainable_variables))\n",
        "      self.a_opt.apply_gradients(zip(grads2, self.actor_main.trainable_variables))\n",
        "      self.c_opt1.apply_gradients(zip(grads3, self.critic_1.trainable_variables))\n",
        "      self.c_opt2.apply_gradients(zip(grads4, self.critic_2.trainable_variables))\n",
        "      \n",
        "      \n",
        "      self.trainstep +=1\n",
        "      #if self.trainstep % self.replace == 0:\n",
        "      self.update_target()     \n",
        "\n",
        "\n",
        "with tf.device('GPU:0'):\n",
        "    tf.random.set_seed(336699)\n",
        "    agent = Agent(2)\n",
        "    episods = 20000\n",
        "    ep_reward = []\n",
        "    total_avgr = []\n",
        "    target = False\n",
        "\n",
        "    for s in range(episods):\n",
        "      if target == True:\n",
        "        break\n",
        "      total_reward = 0 \n",
        "      state = env.reset()\n",
        "      done = False\n",
        "\n",
        "      while not done:\n",
        "        #env.render()\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.savexp(state, next_state, action, done, reward)\n",
        "        agent.train()\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            ep_reward.append(total_reward)\n",
        "            avg_reward = np.mean(ep_reward[-100:])\n",
        "            total_avgr.append(avg_reward)\n",
        "            print(\"total reward after {} steps is {} and avg reward is {}\".format(s, total_reward, avg_reward))\n",
        "            if int(avg_reward) == 200:\n",
        "              target = True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total reward after 0 steps is -12.76882416551885 and avg reward is -12.76882416551885\n",
            "total reward after 1 steps is -729.9921117679771 and avg reward is -371.380467966748\n",
            "total reward after 2 steps is -488.8246094808231 and avg reward is -410.52851513810634\n",
            "total reward after 3 steps is -176.71799226266637 and avg reward is -352.07588441924634\n",
            "total reward after 4 steps is -145.57406092197948 and avg reward is -310.77551971979295\n",
            "total reward after 5 steps is -215.81107869674486 and avg reward is -294.94811288261826\n",
            "total reward after 6 steps is -191.93726878038746 and avg reward is -280.232278010871\n",
            "total reward after 7 steps is -185.0002071783877 and avg reward is -268.3282691568106\n",
            "total reward after 8 steps is -77.98376700294804 and avg reward is -247.17888002860366\n",
            "total reward after 9 steps is -1.2995576044298218 and avg reward is -222.5909477861863\n",
            "total reward after 10 steps is -38.23102744836596 and avg reward is -205.83095502820262\n",
            "total reward after 11 steps is -222.665721486196 and avg reward is -207.23385223303544\n",
            "total reward after 12 steps is -203.10646355565066 and avg reward is -206.91636079631354\n",
            "total reward after 13 steps is -174.9410601857859 and avg reward is -204.6324107527044\n",
            "total reward after 14 steps is -200.47473652111424 and avg reward is -204.3552324705984\n",
            "total reward after 15 steps is -402.72206610465383 and avg reward is -216.75315957272682\n",
            "total reward after 16 steps is -281.2996913586755 and avg reward is -220.55001438366497\n",
            "total reward after 17 steps is -193.73405300878116 and avg reward is -219.06023875172698\n",
            "total reward after 18 steps is -185.1676700951921 and avg reward is -217.2764193487515\n",
            "total reward after 19 steps is -24.06817038723318 and avg reward is -207.6160069006756\n",
            "total reward after 20 steps is -50.73063541743723 and avg reward is -200.14527492528327\n",
            "total reward after 21 steps is -45.40497715225372 and avg reward is -193.11162502650922\n",
            "total reward after 22 steps is -6.633993767265141 and avg reward is -185.00390192828124\n",
            "total reward after 23 steps is -43.404451778027045 and avg reward is -179.10392483868728\n",
            "total reward after 24 steps is -100.18313501563695 and avg reward is -175.94709324576527\n",
            "total reward after 25 steps is -23.78601157924528 and avg reward is -170.09474395089913\n",
            "total reward after 26 steps is -150.8787433907347 and avg reward is -169.3830402264486\n",
            "total reward after 27 steps is -13.106021732295654 and avg reward is -163.8017181373717\n",
            "total reward after 28 steps is -46.146071024427506 and avg reward is -159.744626857615\n",
            "total reward after 29 steps is -65.52237876350743 and avg reward is -156.6038852544781\n",
            "total reward after 30 steps is 3.0956336053606917 and avg reward is -151.45228787190266\n",
            "total reward after 31 steps is -136.31759535891436 and avg reward is -150.9793287308717\n",
            "total reward after 32 steps is -6.963717532732208 and avg reward is -146.61521930062509\n",
            "total reward after 33 steps is -20.248748670197905 and avg reward is -142.89855839973015\n",
            "total reward after 34 steps is -16.950786104382995 and avg reward is -139.30005061986307\n",
            "total reward after 35 steps is -11.41236824697379 and avg reward is -135.74761499839394\n",
            "total reward after 36 steps is 4.7348605330115365 and avg reward is -131.95079133538297\n",
            "total reward after 37 steps is -25.280171938469632 and avg reward is -129.1436697723063\n",
            "total reward after 38 steps is 0.36486527363577215 and avg reward is -125.82293810446164\n",
            "total reward after 39 steps is 10.734918640742183 and avg reward is -122.40899168583155\n",
            "total reward after 40 steps is -286.9378436144087 and avg reward is -126.42189051335782\n",
            "total reward after 41 steps is -10.640423301087871 and avg reward is -123.66518891306569\n",
            "total reward after 42 steps is 5.669101798249434 and avg reward is -120.65741471047697\n",
            "total reward after 43 steps is 43.781849504518085 and avg reward is -116.92015870559071\n",
            "total reward after 44 steps is -7.579405715037661 and avg reward is -114.49036419468953\n",
            "total reward after 45 steps is -48.50075756579939 and avg reward is -113.05580752884408\n",
            "total reward after 46 steps is -16.125339236754847 and avg reward is -110.99345713965072\n",
            "total reward after 47 steps is -48.668778710106736 and avg reward is -109.69502633903521\n",
            "total reward after 48 steps is 8.308733796216156 and avg reward is -107.28678633627497\n",
            "total reward after 49 steps is 17.64826401612375 and avg reward is -104.788085329227\n",
            "total reward after 50 steps is 20.64924203160055 and avg reward is -102.3285298907794\n",
            "total reward after 51 steps is 34.02355887161534 and avg reward is -99.70637433765643\n",
            "total reward after 52 steps is -33.998036909343924 and avg reward is -98.46659438617883\n",
            "total reward after 53 steps is 32.52945327434881 and avg reward is -96.0407416517246\n",
            "total reward after 54 steps is 81.49716579182083 and avg reward is -92.8127796982056\n",
            "total reward after 55 steps is -36.98288722965214 and avg reward is -91.81581733269572\n",
            "total reward after 56 steps is 59.22692556778516 and avg reward is -89.16594465023115\n",
            "total reward after 57 steps is 33.42716258529907 and avg reward is -87.0522703875496\n",
            "total reward after 58 steps is 257.75486745468106 and avg reward is -81.20808161056263\n",
            "total reward after 59 steps is 147.75261625531166 and avg reward is -77.39206997946472\n",
            "total reward after 60 steps is 79.42868859400768 and avg reward is -74.82123787170289\n",
            "total reward after 61 steps is 21.84992597189772 and avg reward is -73.26202555164483\n",
            "total reward after 62 steps is 61.56139028945325 and avg reward is -71.12197133194486\n",
            "total reward after 63 steps is -17.72716635865926 and avg reward is -70.28767750423725\n",
            "total reward after 64 steps is -13.566581074003611 and avg reward is -69.41504525146443\n",
            "total reward after 65 steps is 65.80697006542323 and avg reward is -67.36622683757219\n",
            "total reward after 66 steps is 59.292558900074354 and avg reward is -65.47579719969687\n",
            "total reward after 67 steps is -24.648330569711277 and avg reward is -64.87539327866767\n",
            "total reward after 68 steps is -17.402965852687903 and avg reward is -64.18738708408826\n",
            "total reward after 69 steps is 58.14154987688291 and avg reward is -62.43983084178866\n",
            "total reward after 70 steps is -232.251852266354 and avg reward is -64.83154945340226\n",
            "total reward after 71 steps is 85.28749896988434 and avg reward is -62.746562669745494\n",
            "total reward after 72 steps is 96.62791910798832 and avg reward is -60.56335059059845\n",
            "total reward after 73 steps is -22.370158483748 and avg reward is -60.04722637293832\n",
            "total reward after 74 steps is 105.99484916287648 and avg reward is -57.83333203246079\n",
            "total reward after 75 steps is 60.24721172445067 and avg reward is -56.27964066723827\n",
            "total reward after 76 steps is 64.70659775265143 and avg reward is -54.70839081762931\n",
            "total reward after 77 steps is -21.68541134232859 and avg reward is -54.285019285894684\n",
            "total reward after 78 steps is -54.85200965710351 and avg reward is -54.29219637920112\n",
            "total reward after 79 steps is 146.94463134868963 and avg reward is -51.77673603260249\n",
            "total reward after 80 steps is 135.11750079796946 and avg reward is -49.46939977543494\n",
            "total reward after 81 steps is 85.72455811625836 and avg reward is -47.8206929718777\n",
            "total reward after 82 steps is 107.45430668219987 and avg reward is -45.94990984351532\n",
            "total reward after 83 steps is 95.63678760440031 and avg reward is -44.264353921516324\n",
            "total reward after 84 steps is 82.80279777158933 and avg reward is -42.76944625453861\n",
            "total reward after 85 steps is 18.072275241040156 and avg reward is -42.06198437668304\n",
            "total reward after 86 steps is 1.9917326151425363 and avg reward is -41.55561981355861\n",
            "total reward after 87 steps is 102.42308611132027 and avg reward is -39.91949815532136\n",
            "total reward after 88 steps is -26.141519863484483 and avg reward is -39.76468941046926\n",
            "total reward after 89 steps is 61.271837983507986 and avg reward is -38.64206132831396\n",
            "total reward after 90 steps is 74.5226958003158 and avg reward is -37.39849256865868\n",
            "total reward after 91 steps is 38.10788860911312 and avg reward is -36.57777103411769\n",
            "total reward after 92 steps is 47.40764553618996 and avg reward is -35.67470203873803\n",
            "total reward after 93 steps is 22.63893575183681 and avg reward is -35.05434418990213\n",
            "total reward after 94 steps is 104.35409939104447 and avg reward is -33.58688688905006\n",
            "total reward after 95 steps is 98.58144796556746 and avg reward is -32.21013340098113\n",
            "total reward after 96 steps is 137.3152548500523 and avg reward is -30.462448986022018\n",
            "total reward after 97 steps is 96.85173022160365 and avg reward is -29.163324708393183\n",
            "total reward after 98 steps is 61.81068666050149 and avg reward is -28.244395300626568\n",
            "total reward after 99 steps is 141.58542814642075 and avg reward is -26.546097066156094\n",
            "total reward after 100 steps is 56.640627151813256 and avg reward is -25.852002552982782\n",
            "total reward after 101 steps is 97.90777567920905 and avg reward is -17.573003678510915\n",
            "total reward after 102 steps is -278.9993459298703 and avg reward is -15.474751043001392\n",
            "total reward after 103 steps is 114.94374109271453 and avg reward is -12.558133709447583\n",
            "total reward after 104 steps is 146.45687486423688 and avg reward is -9.63782435158542\n",
            "total reward after 105 steps is 98.88609641848365 and avg reward is -6.490852600433136\n",
            "total reward after 106 steps is 99.47249302699795 and avg reward is -3.5767549823592786\n",
            "total reward after 107 steps is 131.36745718507092 and avg reward is -0.413078338724693\n",
            "total reward after 108 steps is 100.05412853682867 and avg reward is 1.3673006166730732\n",
            "total reward after 109 steps is 108.5910953411877 and avg reward is 2.4662071461292485\n",
            "total reward after 110 steps is -26.87661756295217 and avg reward is 2.5797512449833864\n",
            "total reward after 111 steps is 93.93986970960245 and avg reward is 5.745807156941371\n",
            "total reward after 112 steps is 51.06015631040481 and avg reward is 8.287473355601925\n",
            "total reward after 113 steps is -33.66971980933816 and avg reward is 9.700186759366401\n",
            "total reward after 114 steps is 25.91644505334893 and avg reward is 11.964098575111034\n",
            "total reward after 115 steps is 80.87876613215823 and avg reward is 16.800106897479154\n",
            "total reward after 116 steps is 58.380994707992905 and avg reward is 20.196913758145836\n",
            "total reward after 117 steps is -12.210515689570684 and avg reward is 22.01214913133794\n",
            "total reward after 118 steps is 117.54699158307874 and avg reward is 25.039295748120654\n",
            "total reward after 119 steps is 14.381043018043128 and avg reward is 25.423787882173418\n",
            "total reward after 120 steps is 131.95993349577722 and avg reward is 27.25069357130556\n",
            "total reward after 121 steps is 49.98252497565446 and avg reward is 28.204568592584636\n",
            "total reward after 122 steps is 84.5118784332373 and avg reward is 29.116027314589658\n",
            "total reward after 123 steps is 11.48553380296193 and avg reward is 29.66492717039955\n",
            "total reward after 124 steps is 78.0684768419147 and avg reward is 31.447443288975073\n",
            "total reward after 125 steps is -33.13291427384054 and avg reward is 31.35397426202912\n",
            "total reward after 126 steps is 123.53829795513661 and avg reward is 34.09814467548783\n",
            "total reward after 127 steps is 106.14199210342926 and avg reward is 35.29062481384508\n",
            "total reward after 128 steps is 119.23066170391793 and avg reward is 36.94439214112854\n",
            "total reward after 129 steps is 73.64665084339927 and avg reward is 38.336082437197604\n",
            "total reward after 130 steps is 97.42214960229134 and avg reward is 39.27934759716691\n",
            "total reward after 131 steps is 89.18558041460551 and avg reward is 41.5343793549021\n",
            "total reward after 132 steps is 81.21652641576047 and avg reward is 42.41618179438703\n",
            "total reward after 133 steps is 122.04377841943007 and avg reward is 43.839107065283315\n",
            "total reward after 134 steps is 78.50818746135752 and avg reward is 44.793696800940715\n",
            "total reward after 135 steps is 92.61660394393957 and avg reward is 45.833986522849855\n",
            "total reward after 136 steps is 70.90882804552105 and avg reward is 46.49572619797494\n",
            "total reward after 137 steps is 134.53412440275832 and avg reward is 48.09386916138723\n",
            "total reward after 138 steps is 64.8504113451207 and avg reward is 48.73872462210207\n",
            "total reward after 139 steps is 111.04208937907808 and avg reward is 49.74179632948543\n",
            "total reward after 140 steps is 102.3052512901164 and avg reward is 53.63422727853069\n",
            "total reward after 141 steps is 35.83899288040383 and avg reward is 54.099021440345595\n",
            "total reward after 142 steps is 105.4678007333074 and avg reward is 55.09700842969618\n",
            "total reward after 143 steps is 124.72551246573951 and avg reward is 55.906445059308396\n",
            "total reward after 144 steps is 127.72325450714762 and avg reward is 57.25947166153024\n",
            "total reward after 145 steps is 96.2016341371262 and avg reward is 58.706495578559505\n",
            "total reward after 146 steps is 121.86026556694719 and avg reward is 60.086351626596525\n",
            "total reward after 147 steps is 95.76239062552165 and avg reward is 61.53066331995281\n",
            "total reward after 148 steps is 69.21418842977977 and avg reward is 62.139717866288436\n",
            "total reward after 149 steps is 161.814597942203 and avg reward is 63.58138120554924\n",
            "total reward after 150 steps is 128.10438027426537 and avg reward is 64.65593258797588\n",
            "total reward after 151 steps is 15.49174572327352 and avg reward is 64.47061445649247\n",
            "total reward after 152 steps is 100.24748264788715 and avg reward is 65.81306965206478\n",
            "total reward after 153 steps is 7.5157303783407015 and avg reward is 65.5629324231047\n",
            "total reward after 154 steps is 87.07965268424906 and avg reward is 65.61875729202897\n",
            "total reward after 155 steps is 110.70671545409756 and avg reward is 67.09565331886648\n",
            "total reward after 156 steps is 45.48741326138273 and avg reward is 66.95825819580246\n",
            "total reward after 157 steps is 134.9961557448705 and avg reward is 67.97394812739817\n",
            "total reward after 158 steps is 103.9795339974204 and avg reward is 66.43619479282556\n",
            "total reward after 159 steps is 133.68247464856984 and avg reward is 66.29549337675816\n",
            "total reward after 160 steps is 169.37399220525236 and avg reward is 67.1949464128706\n",
            "total reward after 161 steps is 104.82937858391853 and avg reward is 68.0247409389908\n",
            "total reward after 162 steps is 96.05422588684894 and avg reward is 68.36966929496477\n",
            "total reward after 163 steps is 151.44687965404307 and avg reward is 70.06140975509179\n",
            "total reward after 164 steps is 123.79607072017454 and avg reward is 71.43503627303356\n",
            "total reward after 165 steps is 134.06206351981524 and avg reward is 72.11758720757749\n",
            "total reward after 166 steps is 135.47390723667365 and avg reward is 72.87940069094347\n",
            "total reward after 167 steps is 79.43198685248743 and avg reward is 73.92020386516546\n",
            "total reward after 168 steps is 150.46142899491076 and avg reward is 75.59884781364144\n",
            "total reward after 169 steps is 139.96161685091235 and avg reward is 76.41704848338172\n",
            "total reward after 170 steps is 110.71137429065627 and avg reward is 79.84668074895183\n",
            "total reward after 171 steps is 144.88148323910528 and avg reward is 80.44262059164403\n",
            "total reward after 172 steps is 139.6898685577389 and avg reward is 80.87324008614155\n",
            "total reward after 173 steps is 99.06908324971387 and avg reward is 82.08763250347616\n",
            "total reward after 174 steps is 70.22801670490642 and avg reward is 81.72996417889648\n",
            "total reward after 175 steps is 96.54403131522281 and avg reward is 82.0929323748042\n",
            "total reward after 176 steps is 119.69925991578705 and avg reward is 82.64285899643556\n",
            "total reward after 177 steps is 149.19661026388488 and avg reward is 84.35167921249771\n",
            "total reward after 178 steps is 28.030408736542938 and avg reward is 85.18050339643416\n",
            "total reward after 179 steps is 77.03178995128073 and avg reward is 84.48137498246007\n",
            "total reward after 180 steps is 166.6063885221171 and avg reward is 84.79626385970155\n",
            "total reward after 181 steps is 116.59926562575538 and avg reward is 85.10501093479652\n",
            "total reward after 182 steps is 112.11520667493994 and avg reward is 85.15161993472391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiC-3XrvDUTU"
      },
      "source": [
        "ep = [i  for i in range(len(avg_rewards_list))]\n",
        "plt.plot( range(len(avg_rewards_list)),avg_rewards_list,'b')\n",
        "plt.title(\"Avg Test Aeward Vs Test Episods\")\n",
        "plt.xlabel(\"Test Episods\")\n",
        "plt.ylabel(\"Average Test Reward\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}