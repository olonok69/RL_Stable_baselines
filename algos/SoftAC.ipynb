{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SoftAC.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMGYK/ifipTsptLjr8PN6y+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhisheksuran/Reinforcement_Learning/blob/master/SoftAC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQQaz7ifBP3Z",
        "outputId": "f4ead77e-210d-484d-b1df-01831382a258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow_probability as tfp\n",
        "!pip3 install box2d-py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\r\u001b[K     |▊                               | 10kB 25.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 2.8MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 3.6MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 3.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 3.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 4.5MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efKIDfYGBSUi",
        "outputId": "044285d9-8ec3-4c20-8c5c-82d658a60df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "print(tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "env= gym.make(\"LunarLanderContinuous-v2\")\n",
        "state_low = env.observation_space.low\n",
        "state_high = env.observation_space.high\n",
        "action_low = env.action_space.low \n",
        "action_high = env.action_space.high\n",
        "print(state_low)\n",
        "print(state_high)\n",
        "print(action_low)\n",
        "print(action_high)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "[-inf -inf -inf -inf -inf -inf -inf -inf]\n",
            "[inf inf inf inf inf inf inf inf]\n",
            "[-1. -1.]\n",
            "[1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SwrILlcQpii"
      },
      "source": [
        "class RBuffer():\n",
        "  def __init__(self, maxsize, statedim, naction):\n",
        "    self.cnt = 0\n",
        "    self.maxsize = maxsize\n",
        "    self.state_memory = np.zeros((maxsize, *statedim), dtype=np.float32)\n",
        "    self.action_memory = np.zeros((maxsize, naction), dtype=np.float32)\n",
        "    self.reward_memory = np.zeros((maxsize,), dtype=np.float32)\n",
        "    self.next_state_memory = np.zeros((maxsize, *statedim), dtype=np.float32)\n",
        "    self.done_memory = np.zeros((maxsize,), dtype= np.bool)\n",
        "\n",
        "  def storexp(self, state, next_state, action, done, reward):\n",
        "    index = self.cnt % self.maxsize\n",
        "    self.state_memory[index] = state\n",
        "    self.action_memory[index] = action\n",
        "    self.reward_memory[index] = reward\n",
        "    self.next_state_memory[index] = next_state\n",
        "    self.done_memory[index] = 1- int(done)\n",
        "    self.cnt += 1\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    max_mem = min(self.cnt, self.maxsize)\n",
        "    batch = np.random.choice(max_mem, batch_size, replace= False)  \n",
        "    states = self.state_memory[batch]\n",
        "    next_states = self.next_state_memory[batch]\n",
        "    rewards = self.reward_memory[batch]\n",
        "    actions = self.action_memory[batch]\n",
        "    dones = self.done_memory[batch]\n",
        "    return states, next_states, rewards, actions, dones\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA4t0oizCM0P",
        "outputId": "5a7c774a-6ca4-447c-f460-b2dba28a2a42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "class Critic(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Critic, self).__init__()\n",
        "    self.f1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.f2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.q =  tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "  def call(self, inputstate, action):\n",
        "    x = self.f1(tf.concat([inputstate, action], axis=1))\n",
        "    x = self.f2(x)\n",
        "    x = self.q(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class value_net(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(value_net, self).__init__()\n",
        "    self.f1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.f2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.v =  tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "  def call(self, inputstate):\n",
        "    x = self.f1(inputstate)\n",
        "    x = self.f2(x)\n",
        "    x = self.v(x)\n",
        "    return x\n",
        "    \n",
        "\n",
        "class Actor(tf.keras.Model):\n",
        "  def __init__(self, no_action):\n",
        "    super(Actor, self).__init__()    \n",
        "    self.f1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.f2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.mu =  tf.keras.layers.Dense(no_action, activation=None)\n",
        "    self.sigma =  tf.keras.layers.Dense(no_action, activation=None)\n",
        "    self.min_action = -1\n",
        "    self.max_action = 1\n",
        "    self.repram = 1e-6\n",
        "\n",
        "  def call(self, state):\n",
        "    x = self.f1(state)\n",
        "    x = self.f2(x)\n",
        "    mu = self.mu(x)\n",
        "    s = self.sigma(x)\n",
        "    s = tf.clip_by_value(s, self.repram, 1)\n",
        "    return mu, s\n",
        "    \n",
        "  def sample_normal(self, state, reparameterize=True):\n",
        "    mu, sigma = self(state)\n",
        "    # print(mu)\n",
        "    # print(sigma)\n",
        "    #mu = tf.squeeze(mu)\n",
        "    #sigma =tf.squeeze(sigma)\n",
        "    #print(mu)\n",
        "    probabilities = tfp.distributions.Normal(mu, sigma)\n",
        "    if reparameterize:\n",
        "        actions = probabilities.sample()\n",
        "        #actions += tf.random.normal(shape=tf.shape(actions), mean=0.0, stddev=0.1)\n",
        "        \n",
        "    else:\n",
        "        actions = probabilities.sample()\n",
        "        \n",
        "    action = tf.math.scalar_mul(tf.constant(self.max_action, dtype=tf.float32),tf.math.tanh(actions))\n",
        "    action = tf.squeeze(action)\n",
        "    log_prob = probabilities.log_prob(actions)\n",
        "    log_prob -= tf.math.log(1 - tf.math.pow(action, 2) + self.repram)\n",
        "    log_prob = tf.reduce_sum(log_prob, axis=1)\n",
        "    \n",
        "    return action, log_prob\n",
        "        \n",
        "\n",
        "\n",
        "class Agent():\n",
        "  def __init__(self, n_action= len(env.action_space.high)):\n",
        "    self.actor_main = Actor(n_action)\n",
        "    self.critic_1 = Critic()\n",
        "    self.critic_2 = Critic()\n",
        "    self.value_net = value_net()\n",
        "    self.target_value_net = value_net()\n",
        "    self.batch_size = 64\n",
        "    self.n_actions = len(env.action_space.high)\n",
        "    self.a_opt = tf.keras.optimizers.Adam(0.001)\n",
        "    # self.actor_target = tf.keras.optimizers.Adam(.001)\n",
        "    self.c_opt1 = tf.keras.optimizers.Adam(0.002)\n",
        "    self.c_opt2 = tf.keras.optimizers.Adam(0.002)\n",
        "    self.v_opt = tf.keras.optimizers.Adam(0.002)\n",
        "    # self.critic_target = tf.keras.optimizers.Adam(.002)\n",
        "    self.memory = RBuffer(1_00_000, env.observation_space.shape, len(env.action_space.high))\n",
        "    self.trainstep = 0\n",
        "    #self.replace = 5\n",
        "    self.gamma = 0.99\n",
        "    self.min_action = env.action_space.low[0]\n",
        "    self.max_action = env.action_space.high[0]\n",
        "    self.scale = 2\n",
        "    \n",
        "\n",
        "  def act(self, state):\n",
        "      state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "      action, _ = self.actor_main.sample_normal(state, reparameterize=False)\n",
        "      # print(action)\n",
        "      return action\n",
        "\n",
        "\n",
        "  def savexp(self,state, next_state, action, done, reward):\n",
        "        self.memory.storexp(state, next_state, action, done, reward)\n",
        "\n",
        "  def update_target(self):\n",
        "    self.target_value_net.set_weights(self.value_net.get_weights())\n",
        "  \n",
        "  def train(self):\n",
        "      if self.memory.cnt < self.batch_size:\n",
        "        return \n",
        "\n",
        "\n",
        "      states, next_states, rewards, actions, dones = self.memory.sample(self.batch_size)\n",
        "  \n",
        "      states = tf.convert_to_tensor(states, dtype= tf.float32)\n",
        "      next_states = tf.convert_to_tensor(next_states, dtype= tf.float32)\n",
        "      rewards = tf.convert_to_tensor(rewards, dtype= tf.float32)\n",
        "      actions = tf.convert_to_tensor(actions, dtype= tf.float32)\n",
        "      #dones = tf.convert_to_tensor(dones, dtype= tf.bool)\n",
        "      \n",
        "      \n",
        "      with tf.GradientTape() as tape1, tf.GradientTape() as tape2, tf.GradientTape() as tape3, tf.GradientTape() as tape4:\n",
        "          value = tf.squeeze(self.value_net(states))\n",
        "          #value = self.value_net(states)\n",
        "          #value loss\n",
        "          v_actions, v_log_probs = self.actor_main.sample_normal(states, reparameterize=False)\n",
        "          #print(v_log_probs)\n",
        "          #v_log_probs = tf.squeeze(v_log_probs, 1)\n",
        "          v_q1 = self.critic_1(states, v_actions)\n",
        "          v_q2 = self.critic_2(states, v_actions)\n",
        "          v_critic_value = tf.math.minimum(tf.squeeze(v_q1), tf.squeeze(v_q2))\n",
        "          target_value = v_critic_value - v_log_probs\n",
        "          #print(target_value)   \n",
        "          value_loss = 0.5 * tf.keras.losses.MSE(target_value, value)\n",
        "      \n",
        "          #actor loss\n",
        "          a_actions, a_log_probs = self.actor_main.sample_normal(states, reparameterize=True)\n",
        "          #a_log_probs = tf.squeeze(a_log_probs, 1)\n",
        "          a_q1 = self.critic_1(states, a_actions)\n",
        "          a_q2 = self.critic_2(states, a_actions)\n",
        "          a_critic_value = tf.math.minimum(tf.squeeze(a_q1), tf.squeeze(a_q2))\n",
        "          actor_loss =  a_log_probs - a_critic_value\n",
        "          actor_loss = tf.reduce_mean(actor_loss)\n",
        "    \n",
        "          next_state_value = tf.squeeze(self.target_value_net(next_states)) \n",
        "          #next_state_value = self.target_value_net(next_states)\n",
        "          #critic loss          \n",
        "          q_hat = self.scale * rewards + self.gamma * next_state_value * dones\n",
        "          c_q1 = self.critic_1(states, actions)\n",
        "          c_q2 = self.critic_2(states, actions)\n",
        "          critic_loss1 = 0.5 * tf.keras.losses.MSE(q_hat, tf.squeeze(c_q1))\n",
        "          critic_loss2 = 0.5 * tf.keras.losses.MSE(q_hat, tf.squeeze(c_q2))  \n",
        "      \n",
        "      grads1 = tape1.gradient(value_loss, self.value_net.trainable_variables)\n",
        "      grads2 = tape2.gradient(actor_loss, self.actor_main.trainable_variables)\n",
        "      grads3 = tape3.gradient(critic_loss1, self.critic_1.trainable_variables)\n",
        "      grads4 = tape4.gradient(critic_loss2, self.critic_2.trainable_variables)\n",
        "      self.v_opt.apply_gradients(zip(grads1, self.value_net.trainable_variables))\n",
        "      self.a_opt.apply_gradients(zip(grads2, self.actor_main.trainable_variables))\n",
        "      self.c_opt1.apply_gradients(zip(grads3, self.critic_1.trainable_variables))\n",
        "      self.c_opt2.apply_gradients(zip(grads4, self.critic_2.trainable_variables))\n",
        "      \n",
        "      \n",
        "      self.trainstep +=1\n",
        "      #if self.trainstep % self.replace == 0:\n",
        "      self.update_target()     \n",
        "\n",
        "\n",
        "with tf.device('GPU:0'):\n",
        "    tf.random.set_seed(336699)\n",
        "    agent = Agent(2)\n",
        "\n",
        "    episods = 20000\n",
        "    ep_reward = []\n",
        "    total_avgr = []\n",
        "    target = False\n",
        "\n",
        "    for s in range(episods):\n",
        "      if target == True:\n",
        "        break\n",
        "      total_reward = 0 \n",
        "      state = env.reset()\n",
        "      done = False\n",
        "\n",
        "      while not done:\n",
        "        #env.render()\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.savexp(state, next_state, action, done, reward)\n",
        "        agent.train()\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            ep_reward.append(total_reward)\n",
        "            avg_reward = np.mean(ep_reward[-100:])\n",
        "            total_avgr.append(avg_reward)\n",
        "            print(\"total reward after {} steps is {} and avg reward is {}\".format(s, total_reward, avg_reward))\n",
        "            if avg_reward == 200:\n",
        "              target = True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total reward after 0 steps is -145.81552138196787 and avg reward is -145.81552138196787\n",
            "total reward after 1 steps is -296.4136774632111 and avg reward is -221.1145994225895\n",
            "total reward after 2 steps is -3.527452098095253 and avg reward is -148.58555031442475\n",
            "total reward after 3 steps is -415.83201978128847 and avg reward is -215.39716768114067\n",
            "total reward after 4 steps is -251.8780666280109 and avg reward is -222.6933474705147\n",
            "total reward after 5 steps is -30.481046798220262 and avg reward is -190.6579640251323\n",
            "total reward after 6 steps is -420.9063128053131 and avg reward is -223.55058527944382\n",
            "total reward after 7 steps is -26.68810841659365 and avg reward is -198.94277567158758\n",
            "total reward after 8 steps is -61.2133566616459 and avg reward is -183.63950689270519\n",
            "total reward after 9 steps is -92.19366621088668 and avg reward is -174.49492282452334\n",
            "total reward after 10 steps is -77.37672715869016 and avg reward is -165.66599594581123\n",
            "total reward after 11 steps is -182.90225378496422 and avg reward is -167.10235076574062\n",
            "total reward after 12 steps is -47.81886349010064 and avg reward is -157.9266978983837\n",
            "total reward after 13 steps is -9.150377545520556 and avg reward is -147.2998178731792\n",
            "total reward after 14 steps is 8.901880366335504 and avg reward is -136.88637132387822\n",
            "total reward after 15 steps is 49.05027652053692 and avg reward is -125.26533083360228\n",
            "total reward after 16 steps is -73.83487238069306 and avg reward is -122.24000974813704\n",
            "total reward after 17 steps is -109.35616610384442 and avg reward is -121.52424065678744\n",
            "total reward after 18 steps is 137.78171776623546 and avg reward is -107.87655863452308\n",
            "total reward after 19 steps is 93.70999971704104 and avg reward is -97.79723071694488\n",
            "total reward after 20 steps is -217.87287866577086 and avg reward is -103.51511871450802\n",
            "total reward after 21 steps is -163.14029915850057 and avg reward is -106.22535418923495\n",
            "total reward after 22 steps is -3.416870097355641 and avg reward is -101.75542009828368\n",
            "total reward after 23 steps is -56.03842080159903 and avg reward is -99.85054512758848\n",
            "total reward after 24 steps is -60.32837424071467 and avg reward is -98.26965829211353\n",
            "total reward after 25 steps is -36.146119139555374 and avg reward is -95.88029140163053\n",
            "total reward after 26 steps is -37.35142349254928 and avg reward is -93.71255555314603\n",
            "total reward after 27 steps is -39.39275706740126 and avg reward is -91.77256275008372\n",
            "total reward after 28 steps is 99.49226096248049 and avg reward is -85.17722400137461\n",
            "total reward after 29 steps is 56.23768875688536 and avg reward is -80.46339357609928\n",
            "total reward after 30 steps is 153.39080366865048 and avg reward is -72.91970979401059\n",
            "total reward after 31 steps is 214.77882525471858 and avg reward is -63.92913057373779\n",
            "total reward after 32 steps is 165.97189096308847 and avg reward is -56.96243295140972\n",
            "total reward after 33 steps is 13.262757863959518 and avg reward is -54.896986162722385\n",
            "total reward after 34 steps is 26.29017934902204 and avg reward is -52.57735286238683\n",
            "total reward after 35 steps is -80.49741357417092 and avg reward is -53.35291010438083\n",
            "total reward after 36 steps is 124.4416103884758 and avg reward is -48.547652793763085\n",
            "total reward after 37 steps is -29.731857997641143 and avg reward is -48.0525002991283\n",
            "total reward after 38 steps is -7.018190175406652 and avg reward is -47.00033850108416\n",
            "total reward after 39 steps is -32.82392424770273 and avg reward is -46.64592814474962\n",
            "total reward after 40 steps is -55.391442883033484 and avg reward is -46.85923338226874\n",
            "total reward after 41 steps is 221.0270496575473 and avg reward is -40.48098854798741\n",
            "total reward after 42 steps is -34.10007865156087 and avg reward is -40.33259529458214\n",
            "total reward after 43 steps is -17.146001883520302 and avg reward is -39.80562726251255\n",
            "total reward after 44 steps is -50.25068334663778 and avg reward is -40.037739619937554\n",
            "total reward after 45 steps is -11.269421443309 and avg reward is -39.4123413987065\n",
            "total reward after 46 steps is -199.682022364383 and avg reward is -42.822334610742175\n",
            "total reward after 47 steps is 226.1928126398854 and avg reward is -37.217852376354095\n",
            "total reward after 48 steps is -76.76882154595803 and avg reward is -38.02501501246846\n",
            "total reward after 49 steps is -30.927209417217632 and avg reward is -37.88305890056344\n",
            "total reward after 50 steps is 212.7640228862632 and avg reward is -32.96841023807665\n",
            "total reward after 51 steps is 43.65617765383319 and avg reward is -31.494860470924536\n",
            "total reward after 52 steps is -114.90095595226512 and avg reward is -33.068560385666814\n",
            "total reward after 53 steps is -54.961749403999434 and avg reward is -33.47398981193223\n",
            "total reward after 54 steps is -231.42797672653802 and avg reward is -37.07315321037961\n",
            "total reward after 55 steps is -49.58104629486296 and avg reward is -37.29650844403109\n",
            "total reward after 56 steps is -169.75629511490033 and avg reward is -39.62036435053757\n",
            "total reward after 57 steps is -165.62354619687346 and avg reward is -41.7928330030606\n",
            "total reward after 58 steps is -48.10544407524993 and avg reward is -41.89982641106381\n",
            "total reward after 59 steps is 210.8313228706663 and avg reward is -37.68764058970165\n",
            "total reward after 60 steps is 92.01968334530099 and avg reward is -35.561291016996684\n",
            "total reward after 61 steps is 203.07496479429716 and avg reward is -31.712319149072595\n",
            "total reward after 62 steps is 222.27120388243156 and avg reward is -27.680834656509035\n",
            "total reward after 63 steps is 160.80179076742468 and avg reward is -24.735793634260062\n",
            "total reward after 64 steps is 197.13697179288994 and avg reward is -21.322366473842372\n",
            "total reward after 65 steps is 204.71273525133046 and avg reward is -17.897592205279146\n",
            "total reward after 66 steps is 169.5497401159938 and avg reward is -15.099870827349699\n",
            "total reward after 67 steps is 219.42744922603356 and avg reward is -11.650939650094061\n",
            "total reward after 68 steps is 238.62412796236202 and avg reward is -8.023764757159917\n",
            "total reward after 69 steps is 253.99131203102937 and avg reward is -4.280692231614356\n",
            "total reward after 70 steps is 170.65608366177946 and avg reward is -1.8167939795947248\n",
            "total reward after 71 steps is -122.00524346795758 and avg reward is -3.486078000266431\n",
            "total reward after 72 steps is 193.51708670966087 and avg reward is -0.7874045110893446\n",
            "total reward after 73 steps is 259.9538589262559 and avg reward is 2.7361260759018076\n",
            "total reward after 74 steps is -19.128726347978102 and avg reward is 2.4445947102500756\n",
            "total reward after 75 steps is 245.4421938211786 and avg reward is 5.641931540657029\n",
            "total reward after 76 steps is -110.35080020238055 and avg reward is 4.1355324271110865\n",
            "total reward after 77 steps is 205.51991089793086 and avg reward is 6.717383433147236\n",
            "total reward after 78 steps is 233.39239119005208 and avg reward is 9.58668732880426\n",
            "total reward after 79 steps is 124.78926645834618 and avg reward is 11.026719567923532\n",
            "total reward after 80 steps is 264.1012981908109 and avg reward is 14.15109708178634\n",
            "total reward after 81 steps is 251.8662013091938 and avg reward is 17.05006176748643\n",
            "total reward after 82 steps is 163.37787557341753 and avg reward is 18.813047475991624\n",
            "total reward after 83 steps is 133.3567245157551 and avg reward is 20.17666267884595\n",
            "total reward after 84 steps is 146.58716100713775 and avg reward is 21.66384501211997\n",
            "total reward after 85 steps is 131.4899709547342 and avg reward is 22.94089298819688\n",
            "total reward after 86 steps is 207.35860099554657 and avg reward is 25.060636758396303\n",
            "total reward after 87 steps is 224.66683114307284 and avg reward is 27.328888967313084\n",
            "total reward after 88 steps is -27.116723910472142 and avg reward is 26.71714050801213\n",
            "total reward after 89 steps is 250.25428448828688 and avg reward is 29.200886552237403\n",
            "total reward after 90 steps is 174.732918908483 and avg reward is 30.80013965505329\n",
            "total reward after 91 steps is 231.7850573501074 and avg reward is 32.984758325651704\n",
            "total reward after 92 steps is 181.12865077641516 and avg reward is 34.57770340576744\n",
            "total reward after 93 steps is 225.07049172668974 and avg reward is 36.6042224304581\n",
            "total reward after 94 steps is 224.0455091185924 and avg reward is 38.57728860612267\n",
            "total reward after 95 steps is 228.31229567616023 and avg reward is 40.553694929768895\n",
            "total reward after 96 steps is 216.10165559906017 and avg reward is 42.363467720173965\n",
            "total reward after 97 steps is 245.13608676261867 and avg reward is 44.432576077749935\n",
            "total reward after 98 steps is 172.87061826259793 and avg reward is 45.729930039213045\n",
            "total reward after 99 steps is 273.5543594510728 and avg reward is 48.008174333331645\n",
            "total reward after 100 steps is 251.71725212887958 and avg reward is 51.98350206844011\n",
            "total reward after 101 steps is 87.18121256869156 and avg reward is 55.81945096875915\n",
            "total reward after 102 steps is 236.08530271523185 and avg reward is 58.21557851689241\n",
            "total reward after 103 steps is 236.05882322068953 and avg reward is 64.73448694691218\n",
            "total reward after 104 steps is -216.7717978877277 and avg reward is 65.08554963431502\n",
            "total reward after 105 steps is 169.29690442877853 and avg reward is 67.083329146585\n",
            "total reward after 106 steps is 281.5388640184721 and avg reward is 74.10778091482284\n",
            "total reward after 107 steps is 225.35603721807848 and avg reward is 76.62822237116957\n",
            "total reward after 108 steps is 210.24983045950518 and avg reward is 79.3428542423811\n",
            "total reward after 109 steps is 196.16435849956292 and avg reward is 82.2264344894856\n",
            "total reward after 110 steps is -63.23434993232798 and avg reward is 82.3678582617492\n",
            "total reward after 111 steps is 197.71921945518437 and avg reward is 86.17407299415068\n",
            "total reward after 112 steps is 226.7296397864558 and avg reward is 88.91955802691626\n",
            "total reward after 113 steps is 81.64106102230394 and avg reward is 89.8274724125945\n",
            "total reward after 114 steps is 144.88048269978367 and avg reward is 91.187258435929\n",
            "total reward after 115 steps is 158.39388521740585 and avg reward is 92.28069452289769\n",
            "total reward after 116 steps is 200.76084003949714 and avg reward is 95.02665164709957\n",
            "total reward after 117 steps is 179.28198335654764 and avg reward is 97.9130331417035\n",
            "total reward after 118 steps is 275.061752919838 and avg reward is 99.28583349323952\n",
            "total reward after 119 steps is 216.00869145416289 and avg reward is 100.50882041061074\n",
            "total reward after 120 steps is 261.26904340859573 and avg reward is 105.3002396313544\n",
            "total reward after 121 steps is 240.88910845809096 and avg reward is 109.34053370752034\n",
            "total reward after 122 steps is 268.2368591007546 and avg reward is 112.05707099950145\n",
            "total reward after 123 steps is 118.1366564176356 and avg reward is 113.79882177169377\n",
            "total reward after 124 steps is 171.61068033076708 and avg reward is 116.11821231740858\n",
            "total reward after 125 steps is 107.15253089839304 and avg reward is 117.55119881778806\n",
            "total reward after 126 steps is 219.49459389083086 and avg reward is 120.11965899162186\n",
            "total reward after 127 steps is 151.92655190839065 and avg reward is 122.03285208137979\n",
            "total reward after 128 steps is -296.22030044051616 and avg reward is 118.07572646734984\n",
            "total reward after 129 steps is 238.69852539309042 and avg reward is 119.90033483371188\n",
            "total reward after 130 steps is 182.9276752064768 and avg reward is 120.19570354909014\n",
            "total reward after 131 steps is -1.0416603588655207 and avg reward is 118.03749869295429\n",
            "total reward after 132 steps is 283.6970669595881 and avg reward is 119.21475045291929\n",
            "total reward after 133 steps is 229.85491761268136 and avg reward is 121.38067205040652\n",
            "total reward after 134 steps is 6.111628022609953 and avg reward is 121.1788865371424\n",
            "total reward after 135 steps is 217.86469223459596 and avg reward is 124.16250759523007\n",
            "total reward after 136 steps is 98.93913303576657 and avg reward is 123.90748282170298\n",
            "total reward after 137 steps is 232.80603298458612 and avg reward is 126.53286173152526\n",
            "total reward after 138 steps is 233.68655486609978 and avg reward is 128.9399091819403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiC-3XrvDUTU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
