{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPWDxP7onpVK"
      },
      "source": [
        "# Twin Delayed Deep Deterministic Policy Gradient (TD3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1KRkwmfn7Ox"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Akb9bzn-Sg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26accee8-9e8b-41f1-bd86-85b5cd83b78e"
      },
      "source": [
        "!pip install tensorboardX\n",
        "!pip install gym\n",
        "!pip install roboschool"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\r\u001b[K     |█                               | 10kB 16.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 21.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 10.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (51.3.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Collecting roboschool\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/d4/014e2589708f6b49b75215e5404559dd0ce76e301aed01ae9efa2dc1bfe5/roboschool-1.0.49-cp36-cp36m-manylinux1_x86_64.whl (48.9MB)\n",
            "\u001b[K     |████████████████████████████████| 48.9MB 65kB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from roboschool) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->roboschool) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->roboschool) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->roboschool) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->roboschool) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->roboschool) (0.16.0)\n",
            "Installing collected packages: roboschool\n",
            "Successfully installed roboschool-1.0.49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma5F4MOknpVM"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "5GuSY64shvIX",
        "outputId": "7a7f7b91-2342-4e99-9e73-a8b89ec31d75"
      },
      "source": [
        "!pip install roboschool==1.0.48 gym==0.15.4"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting roboschool==1.0.48\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/71/cdd39baf333425ae46e7c1c3c20aa43dc8936916f63f2c2a067607c02a31/roboschool-1.0.48-cp36-cp36m-manylinux1_x86_64.whl (44.9MB)\n",
            "\u001b[K     |████████████████████████████████| 44.9MB 72kB/s \n",
            "\u001b[?25hCollecting gym==0.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/85/a7a462d7796f097027d60f9a62b4e17a0a94dcf12ac2a9f9a913333b11a6/gym-0.15.4.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 34.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.15.0)\n",
            "Collecting pyglet<=1.3.2,>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 31.0MB/s \n",
            "\u001b[?25hCollecting cloudpickle~=1.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.4-cp36-none-any.whl size=1648483 sha256=68380cd5e78587d72252a25d536db03e3d30e5550cf5c8cb0ec67da29b7c57cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/26/9b/8a1a6599a91077a938ac4348cc3d3ac84bfab0dbfddeb4c6e7\n",
            "Successfully built gym\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement cloudpickle>=1.3, but you'll have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyglet, cloudpickle, gym, roboschool\n",
            "  Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "  Found existing installation: roboschool 1.0.49\n",
            "    Uninstalling roboschool-1.0.49:\n",
            "      Successfully uninstalled roboschool-1.0.49\n",
            "Successfully installed cloudpickle-1.2.2 gym-0.15.4 pyglet-1.3.2 roboschool-1.0.48\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHXuB5zQnpVO"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import gym\n",
        "import roboschool\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0He4LCpnpVT"
      },
      "source": [
        "# Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP-7T-TRnpVU"
      },
      "source": [
        "def hidden_init(layer):\n",
        "    fan_in = layer.weight.data.size()[0]\n",
        "    lim = 1. / np.sqrt(fan_in)\n",
        "    return (-lim, lim)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmpmK3D4npVZ"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "    \"\"\"Initialize parameters and build model.\n",
        "        Args:\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            max_action (float): highest action to take\n",
        "            seed (int): Random seed\n",
        "            h1_units (int): Number of nodes in first hidden layer\n",
        "            h2_units (int): Number of nodes in second hidden layer\n",
        "            \n",
        "        Return:\n",
        "            action output of network with tanh activation\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, action_dim)\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = self.max_action * torch.tanh(self.l3(x)) \n",
        "        return x\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yGoEuJlnpVf"
      },
      "source": [
        "class Critic(nn.Module):\n",
        "    \"\"\"Initialize parameters and build model.\n",
        "        Args:\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            max_action (float): highest action to take\n",
        "            seed (int): Random seed\n",
        "            h1_units (int): Number of nodes in first hidden layer\n",
        "            h2_units (int): Number of nodes in second hidden layer\n",
        "            \n",
        "        Return:\n",
        "            value output of network \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # Q1 architecture\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, 1)\n",
        "\n",
        "        # Q2 architecture\n",
        "        self.l4 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l5 = nn.Linear(400, 300)\n",
        "        self.l6 = nn.Linear(300, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        xu = torch.cat([x, u], 1)\n",
        "\n",
        "        x1 = F.relu(self.l1(xu))\n",
        "        x1 = F.relu(self.l2(x1))\n",
        "        x1 = self.l3(x1)\n",
        "\n",
        "        x2 = F.relu(self.l4(xu))\n",
        "        x2 = F.relu(self.l5(x2))\n",
        "        x2 = self.l6(x2)\n",
        "        return x1, x2\n",
        "\n",
        "\n",
        "    def Q1(self, x, u):\n",
        "        xu = torch.cat([x, u], 1)\n",
        "\n",
        "        x1 = F.relu(self.l1(xu))\n",
        "        x1 = F.relu(self.l2(x1))\n",
        "        x1 = self.l3(x1)\n",
        "        return x1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2KS-IRHnpVn"
      },
      "source": [
        "# Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQwIQ14VnpV6"
      },
      "source": [
        "# Code based on: \n",
        "# https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
        "\n",
        "# Expects tuples of (state, next_state, action, reward, done)\n",
        "class ReplayBuffer(object):\n",
        "    \"\"\"Buffer to store tuples of experience replay\"\"\"\n",
        "    \n",
        "    def __init__(self, max_size=1000000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            max_size (int): total amount of tuples to store\n",
        "        \"\"\"\n",
        "        \n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "\n",
        "    def add(self, data):\n",
        "        \"\"\"Add experience tuples to buffer\n",
        "        \n",
        "        Args:\n",
        "            data (tuple): experience replay tuple\n",
        "        \"\"\"\n",
        "        \n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.ptr)] = data\n",
        "            self.ptr = (self.ptr + 1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(data)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Samples a random amount of experiences from buffer of batch size\n",
        "        \n",
        "        Args:\n",
        "            batch_size (int): size of sample\n",
        "        \"\"\"\n",
        "        \n",
        "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "        states, actions, next_states, rewards, dones = [], [], [], [], []\n",
        "\n",
        "        for i in ind: \n",
        "            s, a, s_, r, d = self.storage[i]\n",
        "            states.append(np.array(s, copy=False))\n",
        "            actions.append(np.array(a, copy=False))\n",
        "            next_states.append(np.array(s_, copy=False))\n",
        "            rewards.append(np.array(r, copy=False))\n",
        "            dones.append(np.array(d, copy=False))\n",
        "\n",
        "        return np.array(states), np.array(actions), np.array(next_states), np.array(rewards).reshape(-1, 1), np.array(dones).reshape(-1, 1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UDI07-nnpV-"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0l6u_4knpWB"
      },
      "source": [
        "class TD3(object):\n",
        "    \"\"\"Agent class that handles the training of the networks and provides outputs as actions\n",
        "    \n",
        "        Args:\n",
        "            state_dim (int): state size\n",
        "            action_dim (int): action size\n",
        "            max_action (float): highest action to take\n",
        "            device (device): cuda or cpu to process tensors\n",
        "            env (env): gym environment to use\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, max_action, env):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-3)\n",
        "\n",
        "        self.critic = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.env = env\n",
        "\n",
        "\n",
        "        \n",
        "    def select_action(self, state, noise=0.1):\n",
        "        \"\"\"Select an appropriate action from the agent policy\n",
        "        \n",
        "            Args:\n",
        "                state (array): current state of environment\n",
        "                noise (float): how much noise to add to acitons\n",
        "                \n",
        "            Returns:\n",
        "                action (float): action clipped within action range\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        \n",
        "        action = self.actor(state).cpu().data.numpy().flatten()\n",
        "        if noise != 0: \n",
        "            action = (action + np.random.normal(0, noise, size=self.env.action_space.shape[0]))\n",
        "            \n",
        "        return action.clip(self.env.action_space.low, self.env.action_space.high)\n",
        "\n",
        "    \n",
        "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "        \"\"\"Train and update actor and critic networks\n",
        "        \n",
        "            Args:\n",
        "                replay_buffer (ReplayBuffer): buffer for experience replay\n",
        "                iterations (int): how many times to run training\n",
        "                batch_size(int): batch size to sample from replay buffer\n",
        "                discount (float): discount factor\n",
        "                tau (float): soft update for main networks to target networks\n",
        "                \n",
        "            Return:\n",
        "                actor_loss (float): loss from actor network\n",
        "                critic_loss (float): loss from critic network\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        for it in range(iterations):\n",
        "\n",
        "            # Sample replay buffer \n",
        "            x, y, u, r, d = replay_buffer.sample(batch_size)\n",
        "            state = torch.FloatTensor(x).to(device)\n",
        "            action = torch.FloatTensor(u).to(device)\n",
        "            next_state = torch.FloatTensor(y).to(device)\n",
        "            done = torch.FloatTensor(1 - d).to(device)\n",
        "            reward = torch.FloatTensor(r).to(device)\n",
        "\n",
        "            # Select action according to policy and add clipped noise \n",
        "            noise = torch.FloatTensor(u).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            # Compute the target Q value\n",
        "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + (done * discount * target_Q).detach()\n",
        "\n",
        "            # Get current Q estimates\n",
        "            current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "            # Compute critic loss\n",
        "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q) \n",
        "\n",
        "            # Optimize the critic\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "            # Delayed policy updates\n",
        "            if it % policy_freq == 0:\n",
        "\n",
        "                # Compute actor loss\n",
        "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "\n",
        "                # Optimize the actor \n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                # Update the frozen target models\n",
        "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "\n",
        "    def save(self, filename, directory):\n",
        "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "\n",
        "\n",
        "    def load(self, filename=\"best_avg\", directory=\"./saves\"):\n",
        "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ1Kf78_npWE"
      },
      "source": [
        "# Runner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYzKUCOnnpWF"
      },
      "source": [
        "class Runner():\n",
        "    \"\"\"Carries out the environment steps and adds experiences to memory\"\"\"\n",
        "    \n",
        "    def __init__(self, env, agent, replay_buffer):\n",
        "        \n",
        "        self.env = env\n",
        "        self.agent = agent\n",
        "        self.replay_buffer = replay_buffer\n",
        "        self.obs = env.reset()\n",
        "        self.done = False\n",
        "        \n",
        "    def next_step(self, episode_timesteps, noise=0.1):\n",
        "        \n",
        "        action = self.agent.select_action(np.array(self.obs), noise=0.1)\n",
        "        \n",
        "        # Perform action\n",
        "        new_obs, reward, done, _ = self.env.step(action) \n",
        "        done_bool = 0 if episode_timesteps + 1 == 200 else float(done)\n",
        "    \n",
        "        # Store data in replay buffer\n",
        "        replay_buffer.add((self.obs, new_obs, action, reward, done_bool))\n",
        "        \n",
        "        self.obs = new_obs\n",
        "        \n",
        "        if done:\n",
        "            self.obs = self.env.reset()\n",
        "            done = False\n",
        "            \n",
        "            return reward, True\n",
        "        \n",
        "        return reward, done"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhmggP7TnpWH"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g6CYzr2npWI"
      },
      "source": [
        "def evaluate_policy(policy, env, eval_episodes=100,render=False):\n",
        "    \"\"\"run several episodes using the best agent policy\n",
        "        \n",
        "        Args:\n",
        "            policy (agent): agent to evaluate\n",
        "            env (env): gym environment\n",
        "            eval_episodes (int): how many test episodes to run\n",
        "            render (bool): show training\n",
        "        \n",
        "        Returns:\n",
        "            avg_reward (float): average reward over the number of evaluations\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    avg_reward = 0.\n",
        "    for i in range(eval_episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            if render:\n",
        "                env.render()\n",
        "            action = policy.select_action(np.array(obs), noise=0)\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            avg_reward += reward\n",
        "\n",
        "    avg_reward /= eval_episodes\n",
        "\n",
        "    print(\"\\n---------------------------------------\")\n",
        "    print(\"Evaluation over {:d} episodes: {:f}\" .format(eval_episodes, avg_reward))\n",
        "    print(\"---------------------------------------\")\n",
        "    return avg_reward"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5uXjG0inpWL"
      },
      "source": [
        "# Observation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zw822TgnpWM"
      },
      "source": [
        "def observe(env,replay_buffer, observation_steps):\n",
        "    \"\"\"run episodes while taking random actions and filling replay_buffer\n",
        "    \n",
        "        Args:\n",
        "            env (env): gym environment\n",
        "            replay_buffer(ReplayBuffer): buffer to store experience replay\n",
        "            observation_steps (int): how many steps to observe for\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    time_steps = 0\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while time_steps < observation_steps:\n",
        "        action = env.action_space.sample()\n",
        "        new_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "        replay_buffer.add((obs, new_obs, action, reward, done))\n",
        "\n",
        "        obs = new_obs\n",
        "        time_steps += 1\n",
        "\n",
        "        if done:\n",
        "            obs = env.reset()\n",
        "            done = False\n",
        "\n",
        "        print(\"\\rPopulating Buffer {}/{}.\".format(time_steps, observation_steps), end=\"\")\n",
        "        sys.stdout.flush()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZWuqwO1npWP"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6877aX9BnpWQ"
      },
      "source": [
        "def train(agent, test_env):\n",
        "    \"\"\"Train the agent for exploration steps\n",
        "    \n",
        "        Args:\n",
        "            agent (Agent): agent to use\n",
        "            env (environment): gym environment\n",
        "            writer (SummaryWriter): tensorboard writer\n",
        "            exploration (int): how many training steps to run\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    total_timesteps = 0\n",
        "    timesteps_since_eval = 0\n",
        "    episode_num = 0\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    done = False \n",
        "    obs = env.reset()\n",
        "    evaluations = []\n",
        "    rewards = []\n",
        "    best_avg = -2000\n",
        "    \n",
        "    writer = SummaryWriter(comment=\"-TD3_Baseline_HalfCheetah\")\n",
        "    \n",
        "    while total_timesteps < EXPLORATION:\n",
        "    \n",
        "        if done: \n",
        "\n",
        "            if total_timesteps != 0: \n",
        "                rewards.append(episode_reward)\n",
        "                avg_reward = np.mean(rewards[-100:])\n",
        "                \n",
        "                writer.add_scalar(\"avg_reward\", avg_reward, total_timesteps)\n",
        "                writer.add_scalar(\"reward_step\", reward, total_timesteps)\n",
        "                writer.add_scalar(\"episode_reward\", episode_reward, total_timesteps)\n",
        "                \n",
        "                if best_avg < avg_reward:\n",
        "                    best_avg = avg_reward\n",
        "                    print(\"saving best model....\\n\")\n",
        "                    agent.save(\"best_avg\",\"/content/drive/MyDrive/Colab Notebooks/saves\")\n",
        "\n",
        "                print(\"\\rTotal T: {:d} Episode Num: {:d} Reward: {:f} Avg Reward: {:f}\".format(\n",
        "                    total_timesteps, episode_num, episode_reward, avg_reward), end=\"\")\n",
        "                sys.stdout.flush()\n",
        "\n",
        "\n",
        "                if avg_reward >= REWARD_THRESH:\n",
        "                    break\n",
        "\n",
        "                agent.train(replay_buffer, episode_timesteps, BATCH_SIZE, GAMMA, TAU, NOISE, NOISE_CLIP, POLICY_FREQUENCY)\n",
        "\n",
        "                # Evaluate episode\n",
        "                if timesteps_since_eval >= EVAL_FREQUENCY:\n",
        "                    timesteps_since_eval %= EVAL_FREQUENCY\n",
        "                    eval_reward = evaluate_policy(agent, test_env)\n",
        "                    evaluations.append(avg_reward)\n",
        "                    writer.add_scalar(\"eval_reward\", eval_reward, total_timesteps)\n",
        "\n",
        "                    if best_avg < eval_reward:\n",
        "                        best_avg = eval_reward\n",
        "                        print(\"saving best model....\\n\")\n",
        "                        agent.save(\"best_avg\",\"/content/drive/MyDrive/Colab Notebooks/saves\")\n",
        "\n",
        "                episode_reward = 0\n",
        "                episode_timesteps = 0\n",
        "                episode_num += 1 \n",
        "\n",
        "        reward, done = runner.next_step(episode_timesteps)\n",
        "        episode_reward += reward\n",
        "\n",
        "        episode_timesteps += 1\n",
        "        total_timesteps += 1\n",
        "        timesteps_since_eval += 1"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfwWrVMOnpWT"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyOO_23tnpWT"
      },
      "source": [
        "ENV = \"RoboschoolHalfCheetah-v1\"#\"Pendulum-v0\"\n",
        "SEED = 0\n",
        "OBSERVATION = 10000\n",
        "EXPLORATION = 5000000\n",
        "BATCH_SIZE = 100\n",
        "GAMMA = 0.99\n",
        "TAU = 0.005\n",
        "NOISE = 0.2\n",
        "NOISE_CLIP = 0.5\n",
        "EXPLORE_NOISE = 0.1\n",
        "POLICY_FREQUENCY = 2\n",
        "EVAL_FREQUENCY = 5000\n",
        "REWARD_THRESH = 8000"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE5NjQdcnpWV"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgao3UPhnpWW"
      },
      "source": [
        "env = gym.make(ENV).env\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set seeds\n",
        "env.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0] \n",
        "max_action = float(env.action_space.high[0])\n",
        "\n",
        "policy = TD3(state_dim, action_dim, max_action, env)\n",
        "\n",
        "replay_buffer = ReplayBuffer()\n",
        "\n",
        "runner = Runner(env, policy, replay_buffer)\n",
        "\n",
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alYTdprknpWY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8db5c98-7293-4023-9225-51f0601ddcb9"
      },
      "source": [
        "# Populate replay buffer\n",
        "observe(env, replay_buffer, OBSERVATION)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating Buffer 10000/10000."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DzONcNZnpWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "852b63ce-4c6b-4197-802b-8e2c4b593731"
      },
      "source": [
        "# Train agent\n",
        "train(policy, env)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saving best model....\n",
            "\n",
            "Total T: 63 Episode Num: 2 Reward: 1.477290 Avg Reward: 7.170434saving best model....\n",
            "\n",
            "Total T: 5004 Episode Num: 172 Reward: 16.198123 Avg Reward: 20.766220\n",
            "---------------------------------------\n",
            "Evaluation over 100 episodes: 13.924559\n",
            "---------------------------------------\n",
            "Total T: 10022 Episode Num: 358 Reward: 15.004981 Avg Reward: 25.360331"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hholys18nJqf",
        "outputId": "1c94cdea-20b0-4d86-dd3f-051080956fd9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp2UFWtpnpWh"
      },
      "source": [
        "policy.load()\n",
        "\n",
        "for i in range(100):\n",
        "    evaluate_policy(policy, env, render=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZLq4d0anpWj"
      },
      "source": [
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxQ4-QrqnpWk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}