{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = env.observation_space.low\n",
    "high = env.observation_space.high\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grids(l,h,bins=(10,10)):\n",
    "    grid = [np.linspace(l[dim],h[dim],bins[dim]+1)[1:-1] for dim in range(len(bins))]\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete(sample_state, grid):\n",
    "    return list(int(np.digitize(s,g)) for s, g in zip(sample_state, grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_grid = create_grids(env.observation_space.low, env.observation_space.high, bins=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_agent():\n",
    "    \n",
    "    def __init__(self,grid,env):\n",
    "            self.env = env\n",
    "            self.grid = grid\n",
    "            self.state_size = tuple(len(i)+1 for i in self.grid)\n",
    "            print(\"State space size:\", self.state_size)\n",
    "            self.action_size = env.action_space.n\n",
    "            self.gamma = 0.99\n",
    "            self.alpha = 0.02\n",
    "            self.epsilon = 1.0\n",
    "            self.decay = 0.0001 \n",
    "            self.min_epsilon = 0.01\n",
    "            self.q_table = np.zeros(shape=(self.state_size + (self.action_size,)))\n",
    "            print(\"Q table size:\", self.q_table.shape)\n",
    "        \n",
    "    def get_state(self,state):\n",
    "            return tuple(discrete(state, self.grid))\n",
    "                     \n",
    "                                          \n",
    "    def _update_epsilon(self):\n",
    "            self.epsilon -= self.decay\n",
    "            self.epsilon = max(self.min_epsilon, self.epsilon)\n",
    "\n",
    "    def act(self, state, is_new_episode):\n",
    "        \n",
    "#             if is_new_episode:\n",
    "#                 return self.env.action_space.sample()\n",
    "                \n",
    "                \n",
    "        \n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                         self._update_epsilon()\n",
    "                         return self.env.action_space.sample()\n",
    "                         \n",
    "            state =  self.get_state(state)\n",
    "            action = np.argmax(self.q_table[state])\n",
    "            return action             \n",
    "                         \n",
    "    def q_update(self,state, action, reward, next_state):\n",
    "                         state =  self.get_state(state)\n",
    "                         next_state = self.get_state(next_state)\n",
    "                         self.q_table[state + (action,)] += self.alpha * \\\n",
    "                        (reward + self.gamma * max(self.q_table[next_state]) - \\\n",
    "                         self.q_table[state + (action,)])\n",
    "                         \n",
    "agent = my_agent(state_grid, env)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_episode = 20_000\n",
    "i = 0\n",
    "for ep in range(total_episode):\n",
    "                done = False\n",
    "                state = env.reset()\n",
    "                total_reward = 0\n",
    "                action = agent.act(state,True)\n",
    "\n",
    "                \n",
    "                i +=1\n",
    "                while not done:\n",
    "                    \n",
    "                    env.render()\n",
    "                    next_state, reward, done, _ = env.step(action)\n",
    "                    agent.q_update(state, action, reward, next_state)\n",
    "                    total_reward += reward   \n",
    "                    state = next_state    \n",
    "                    action = agent.act(state, False)\n",
    "                if done:\n",
    "                           print(\"total reward of episode {} is {}\".format(i,total_reward))\n",
    "                           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_gpuenv]",
   "language": "python",
   "name": "conda-env-tensorflow_gpuenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
