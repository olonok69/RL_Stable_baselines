{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing some lib...:p  KINDLY SEE ANOTHER FILE IN REPO. PLZ LOOK COMMENTs\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import  Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import os\n",
    "from tensorflow.keras.losses import Huber\n",
    "import tqdm\n",
    "from skimage import transform \n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing gpu availability and tensorflow version\n",
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading environment\n",
    "env = gym.make('Breakout-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting idea about environment\n",
    "print(\"The size of our frame is: \", env.observation_space)\n",
    "print(\"The action size is : \", env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting different actions\n",
    "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "print(possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# processing images from environment\n",
    "\n",
    "def image_process(frame):\n",
    "    \n",
    "    gray = rgb2gray(frame)\n",
    "    cropped_frame = gray[8:-12,4:-12]\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [105,80])\n",
    "    return preprocessed_frame \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 images represents a single state\n",
    "\n",
    "\n",
    "stack_size = 4 \n",
    "\n",
    "\n",
    "stacked_frames  =  deque([np.zeros((105,80), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "\n",
    "def stack_state(stacked_frames, state, is_new_episode):\n",
    "    \n",
    "    frame = image_process(state)\n",
    "\n",
    "    if is_new_episode:\n",
    "\n",
    "        stacked_frames = deque([np.zeros((105,80), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    else:\n",
    "\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    stac =  np.array(stacked_state).reshape(-1, 105, 80, 1)      \n",
    "   # print(stac.shape)\n",
    "    return stac, stacked_frames\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our agent 007 :P\n",
    "\n",
    "class my_agent:\n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        self.state_input = Input((105,80,1), name='state_input')\n",
    "        #self.action_input = action\n",
    "        self.gamma = 0.6\n",
    "        self.epsilon = 0.1\n",
    "        self.epsilon_decay = 0.00001 \n",
    "        self.memory = deque(maxlen=50000)\n",
    "        self.q_net = self.main_model()\n",
    "        self.target_net = self.main_model()\n",
    "        self.alighn_target_model()\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        \n",
    "    def main_model(self):\n",
    "        x = Conv2D(16,8,(4,4), activation='relu')(self.state_input)\n",
    "        x = Conv2D(32,4,(2,2), activation = 'relu')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(256)(x)\n",
    "        y = Dense(env.action_space.n)(x)\n",
    "        #q_val = tf.reduce_sum(tf.multiply(y, self.actions_input))\n",
    "        model = Model(inputs=[self.state_input], outputs= y)\n",
    "        optimizer = Adam(learning_rate=0.01)\n",
    "        huber = Huber()\n",
    "        model.compile(optimizer, loss=huber)\n",
    "        return model\n",
    "        \n",
    "        \n",
    "        \n",
    "    def store(self, state, action, reward, next_state, terminated):\n",
    "        self.memory.append((state, action, reward, next_state, terminated))\n",
    "        \n",
    "    \n",
    "    def _update_epsilon(self):\n",
    "        self.epsilon -= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        \n",
    "        \n",
    "   \n",
    "    def alighn_target_model(self):\n",
    "        self.target_net.set_weights(self.q_net.get_weights())     \n",
    "        \n",
    "     \n",
    "    def act(self, state):\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            self._update_epsilon()\n",
    "            return self.env.action_space.sample()\n",
    "        \n",
    "        state= state        \n",
    "        q_values = self.q_net.predict(state)\n",
    "        a = np.argmax(q_values[0])\n",
    "        #print(a)\n",
    "        return np.array([a])\n",
    "        \n",
    "   \n",
    "    def retrain(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, terminated in minibatch:\n",
    "            \n",
    "            state = state  \n",
    "      \n",
    "            next_state = next_state\n",
    "           \n",
    "            \n",
    "            target = self.q_net.predict(state)\n",
    "            #print(target)\n",
    "            if terminated:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_net.predict(next_state)\n",
    "                target[0][action] = reward + self.gamma * np.amax(t)\n",
    "            \n",
    "            self.q_net.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    " \n",
    "     \n",
    "    def load(self, path):\n",
    "        self.q_net.load_weights(path)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.q_net.save_weights(path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "agent = my_agent(env)\n",
    "\n",
    "# some hyperparameter...\n",
    "\n",
    "batch_size = 64                             \n",
    "num_of_episodes = 1000\n",
    "timesteps_per_episode = 1000\n",
    "agent.q_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# our agent will play and learn\n",
    "\n",
    "\n",
    "for e in tqdm.tqdm(range(0, num_of_episodes)):\n",
    "    \n",
    "    state = env.reset()\n",
    "    state , stacked_frames = stack_state(stacked_frames, state, True)\n",
    "\n",
    "    reward = 0\n",
    "    terminated = False\n",
    "\n",
    "    for timestep in range(timesteps_per_episode):\n",
    "        env.render()\n",
    "        state = state \n",
    "        \n",
    "        action = agent.act(state)\n",
    "        #action = np.array([1])\n",
    "        print(action)\n",
    "        \n",
    "        next_state, reward, terminated, info = env.step(action)\n",
    "        next_state , stacked_frames = stack_state(stacked_frames, next_state , False)\n",
    "\n",
    "        agent.store(state, action, reward, next_state, terminated)\n",
    "\n",
    "        state  = next_state\n",
    "\n",
    "\n",
    "        if terminated:\n",
    "         \n",
    "            print(\"Total reward is {}\".format(rewards))\n",
    "            agent.alighn_target_model()\n",
    "            break\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.retrain(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2] *",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
